Welcome to Professor Chomsky's linguistics talk.
We are delighted to have you here.
Thank you for sharing this wonderful and exciting event
with us.
My name is Simin Karimi.
I am the head of the Department of Linguistics.
We and the College of Social and Behavioral Sciences
are truly privileged and honored to have
Noam Chomsky visit our campus.
This is a unique event.
For several years, some of us entertained the idea
of inviting him to the campus.
And finally, over a year ago, a group of three colleagues
in our college-- they achieved this goal
and they arranged his visit.
Two of them are my esteemed colleagues in our department.
Massimo Piattelli-Palmarini is one of them
who has collaborated with Professor Chomsky.
And he is actually a veteran in inviting them
to various conferences.
In 1974, he invited him to a conference on biolinguistics.
And this was probably the beginning
of the field of biolinguistics.
Then, he invited him for a debate with Piaget,
and afterwards to several conferences and events
in Europe and France and Italy.
And each one of those was a success.
The other member of the trio is Tom Bever, a regents' professor
in our department.
And Tom was one of the first graduate students of Chomsky
at MIT-- early 1960s, some 50 years ago.
Tom will be the moderator for the question
and answer tonight.
The third member is professor Al Bergesen, the head
of the Department of Sociology.
And we are grateful to all three of them
for inviting Professor Chomsky.
Professor Chomsky's visit is hosted
by the College of Social and Behavioral Sciences,
Conference Center for Creative Inquiry
and the Department of Linguistics.
Our special thanks to our Dean, J.P. Jones III,
who generally supported this event.
And also to Javier Duran, the director of the Conference
Center for Creative Inquiry.
And you can imagine that students, staff members faculty
members-- in the Department of Linguistics--
they have been enthusiastically involved
in every aspect of this event.
And in fact, many of our graduate students
are on the floor today.
They have volunteered to help the event tonight.
And they will be at the event tomorrow night
as well, helping.
Thank you guys for your help.
Many individuals and units have supported
generously and enthusiastically--
and co-sponsored this event.
We are thankful to all of them, and I'll go through the list ,
and mention each one of them.
At the College of Education, Elise Collins Shields
and Gretchen Shields.
Arizona Daily Star.
School of anthropology.
Department of Computer Science, Cognitive Science Program.
Department of Communication.
School of Geography and Development Department
of Gender and Women's Studies.
School of Government and Public Policy.
Department of History.
College of Humanities.
School of Journalism.
Center for Middle Eastern Studies.
School of Middle Eastern and North African Studies.
Department of Philosophy.
Department of Psychology.
Department of Sociology.
And U of A bookstore.
Many units and individuals on our campus-- they helped.
They worked tirelessly to make this event happen.
We are grateful to them.
All members of the Development Office and the College
of Social and Behavioral Sciences.
The Office of the Dean of Students.
Members of the Student Union Event Planning Office.
And U of A police department.
The usual trend is to introduce the speaker
before he delivers his talk.
This is a special case, though.
Professor Chomsky, according to New York Times,
is one of the most important intellectuals
in the history of human beings, and the only one alive.
So he actually doesn't really need an introduction.
I thought that instead of saying something about him-- you all
know him-- I would share with you some of his famous quotes
about language.
And these quotes will set the stage for his talk.
Here is the first one.
"Learning her mother language is not something
that the child does.
It is something that happens to her."
A second one, the scientific value of the expression, quote,
"learning one's mother language," unquote,
is the same as that of the expression,
quote, "the sun rises," unquote.
It's zero.
And the third one, "Linguistics is
part of biology at the suitable level of abstraction."
And finally, the last one.
"The cases of extreme poverty of the stimulus-- that
means the deaf, the blind, the deaf and blind-- interesting
as they are, mislead us into thinking
that the ordinary child in the ordinary linguistic environment
is not also a case of extreme poverty of stimulus."
The topic of today's talk is "What
is Special About Language?"
Please join me to welcome the legend Noam Chomsky.
[APPLAUSE]
Thank you.
Well, the advantage of sitting out in the back room
is I didn't have to hear what was being said about me,
so I don't feel embarrassed.
Although I probably should be.
The most elementary question that one
can ask about language, the simplest one,
is the one that I selected as the title of the talk.
What is special about language?
How is it different, if at all, from everything else we do?
One possibility-- in fact, the simplest answer--
is that nothing is special about language.
It's just a kind of a random, arbitrary selection from things
we're doing all the time.
That may sound kind of absurd.
And my own feeling is it is absurd.
But it's important, because it's a very widely held position.
In fact, it's, I think, overwhelmingly
the dominant position among people concerned with language.
Now, that sounds like a contradiction,
and I think it is a contradiction,
but it nevertheless is true.
I'll give you some examples from now and from 50 years ago.
And strikingly, they're about the same.
So take, say, the journal Science,
the Journal of the American Academy
for Advancement of Science, the main scientific weekly
in the country.
I mean, professional journal.
There was a recent article, a review article,
that happens to encapsulate a lot of generally accepted
ideas.
So I'll refer to it a couple of times.
It holds that, it suggests that, language is entirely grounded
in a constellation of cognitive capacities,
that each, taken separately, has other functions as well, which
means that language exists only in the sense,
roughly, that, say, today's weather exists.
I mean, today's weather exists, but it's not
a topic of inquiry, of scientific study.
It's just a collection of a lot of things
that happened to interact to give you today's weather.
But each of them is something separate-- the wind
speed, whatever it may be.
That's Science mag.
If you go to the professional literature,
the technical literature, the major journal
of cognitive science is called Cognitive Science.
One recent article, just a couple of issues
ago, is about what the author calls
MOM, more than one method.
The question that the author is raising is this.
He says, well, we know there's one method
for acquiring language, namely, statistical analysis of data.
And is there another method, or does that do everything?
Well, if that does everything, then there's
no language, because statistical analysis would
work for anything else, too.
And he concludes that-- the article
studies an invented example in an artificial language,
and claims-- rightly or wrongly, you
can look at it-- that this example could be handled
by statistical analysis.
So therefore there isn't more than one method.
There's only one method, statistical analysis.
That throws out the window about 30 years,
40 years of intensive research on acquisition
of language, from which a lot has been learned.
But the conclusion is there's nothing there,
because just statistical analysis of data does it.
In fact, that's the conventional view, almost
universal view-- close to it, not entirely--
in computational cognitive science.
And in fact, a good part of the general field,
and incidentally, among linguists as well.
This position is not new.
If you go back 50, 60 years, pretty much the same view
was held.
So the most important, the most prestigious 20th century
American linguist, Leonard Bloomfield,
who wrote the standard book Language, for him, a language
is just a collection of all the sentences
that can be spoken in a speech community.
Well, first of all, "all the sentences" is something.
You can't list all the sentences.
In fact, it's an infinite list.
So it leaves pretty obscure what it is.
But another question is left open--
how do you know if something is a sentence?
How do you know if this action that took place is a sentence,
or it's taking a walk or cooking dinner or something like that?
There's no proposal about that.
The leading philosopher, one of the major philosophers
of 20th century, and certainly the one
who was most concerned with language,
was W. V. Quine of Harvard.
For him, a language is a fabric of sentences associated
with one another and with stimuli
by the mechanism of conditioned response.
So it's supposed to hold for behavior altogether.
So therefore, a language is just some arbitrary collection
of objects-- we'll call them sentences--
which are like anything else that happens.
He also recognized that language has to be infinite.
So separately, he said a language is just
a class of sentences, kind of like Bloomfield's definition.
But that leaves the same question,
apart from being inconsistent with this definition.
Well, within the professional theoretical linguistics
of the time, 1950s, there was about the same view.
One of the major theoretical linguists,
Martin [? Jos, ?] edited a major collection of essays
on structural linguistics, and he
proposed what he called the Boasian position, named
after Frank Boas, great anthropologist.
Whether it's accurate to say this about Boas or not
is another question.
But he advanced the Boasian tradition,
which is that languages can differ arbitrarily
from one another.
So the next time you study a new language,
you cannot come to it with any preconceptions about what
a language might be like, because it could be arbitrarily
different.
That's the way I studied linguistics, incidentally.
That was just taken for granted.
There's further views, which, again, Quine
and others who've followed him, like John Searle,
arguing that not only language but every mental action
has to be accessible to consciousness.
So if you can't introspect and see it,
it's not there, which would make linguistics pretty easy,
incidentally, if that were true.
You'd just sit around and think and you can figure out
how it's working.
But there can't, in principle, be
anything going on in your mind that's not
accessible to consciousness.
The theories of linguistics, such as they were,
were collections of procedures of analysis.
So if you studied linguistics and structural linguistics,
1940s, '50s, when I was a student,
you were taught how to-- you took field methods courses,
in which you were instructed on how
to get data from an informant.
And then you had various methods you
could apply to organize the data in a useful form.
How you did it depended on your purposes.
There was no right way to do it.
So you do it this way if it's pedagogical grammar,
this way if you want to submit it to the International
Journal of American Linguistics, and so on and so forth.
So it was sets of procedures of analysis.
One well-known linguist ridiculed
what he called the God's truth conception,
namely, there's something true that you
could say about language.
There can't be anything like this.
It's just different ways of organizing data.
All of this was in a framework of behavioral science, which
itself was embedded in a very strange misinterpretation
of logical positivism.
That's a whole story in itself.
But anyway, that was the view.
And it also followed from this that there were
no problems about language.
There's nothing puzzling about it.
You just collect the data, you organize it this way, that way,
and you're done.
I remember as a student, we used to-- teenage student
back at Penn in the late '40s, we thought,
this is kind of fun.
But what happens when we've done a phonemic analysis
of every language there is?
That's it.
Field's over.
It's a terminal field.
So what happens next?
And in fact, if you look at the literature,
then what was assumed happens next
is you go on to other fields, to other applications.
All of this seems very strange, I think.
It has been a very common view.
Still is.
Maybe the most common view.
And the other thing is, the obvious evidence against it
just seems overwhelming.
So just take the most obvious phenomena.
For example, take a newborn infant.
The infant is plunged into what William James famously called
blooming, buzzing confusion.
Just a lot of things going on.
How does the infant figure out that some parts of that
are language-relevant?
That's a non-trivial question.
Nobody knows the answer to it.
But somehow, every infant does it reflexively.
And we even know now that it's in part, at least,
done prenatally.
But something is going on in the infant's head that's
taking out of this massive confusion various things
and says, OK, you're language-related.
You're not.
It's not the auditory system.
For one thing, it's now well known
that sign works the same way.
That wasn't known at the time, but now it's
just another language, happens to use a different modality.
Even touch works.
There's striking evidence about that.
Another reason why it can't be the auditory system
is that there is very recent work
which indicates that chimpanzees, at least,
maybe other primates, have approximately
the same auditory system.
Interestingly, it's even tuned to
the same phonetic distinctions that
are used in human languages.
But when a chimpanzee is faced with a blooming, buzzing
confusion, it doesn't take anything out of it.
So some kind of internal computation
has got to be going on from the very first instant.
It's reflexive, of course.
And then a process of acquisition proceeds
with virtually no evidence, often,
no evidence that goes from the meaning of words
up to the structures, interpretation, of sentences,
and the proper ways of using them.
All of this, incidentally, it's all apparently more
or less reflexive.
Doesn't matter-- I mean, within a very wide range,
it doesn't matter how the infant, the child, is raised.
It's like a sponge.
It just comes, like growing, growing legs and arms.
And all of this apparently is dissociated
from other cognitive capacities.
So there's dissociations all across the board.
This just seems to be one cognitive capacity which
develops the way, say, vision develops.
Return to a couple of examples.
It's also interesting that the nonexistence approach,
the standard one, is supported by literally no evidence
at all.
So take, for example, the one concrete proposal
I mentioned from a recent professional journal, MOM,
more than one method.
Well, the one method, which was just taken for granted,
it's been well established that that method doesn't work.
It doesn't do anything.
It can do something, if it's integrated with other facts,
other principles of language.
I won't go into the details, but if you
use these statistical techniques and you integrate them
with material about prosody, pitch and stress,
what's called phonotactics, the way different pieces hang
together and so on, then you can get some answers about, say,
where words are broken up.
Nothing else.
So the one method that's taken for granted doesn't work.
And in fact, I think if you look--
well, talk a little more about this--
there are no results, literally no results, that
come from the nonexistence approach, the approach that
says, we just use general-- the one I
quoted from Science magazine, the constellation
of cognitive capacities.
As far as I'm aware, there are no results from that.
That may sound harsh, and you shouldn't take my word for it.
But I think it is accurate.
And it raises some interesting questions
about intellectual history.
So why do beliefs that have no empirical support, literally,
prevail over long periods, while counterevidence,
which seems to be overwhelming, is simply ignored?
That's an interesting question about our intellectual culture,
the one we live in, the one in the universities.
Well, let me just give you a concrete one.
I don't want to make it totally abstract,
so just take one concrete example to illustrate.
There's myriad examples.
Take the sentence, "He wondered if the mechanics
fixed the cars."
Understand what that means?
And there are a couple of questions
you can ask about that.
You can ask, how many cars?
How many mechanics?
So you can say, how many cars did he
wonder whether-- how many cars did he
wonder if the mechanics fixed?
OK.
Fine sentence.
Try "mechanics."
How many mechanics did he wonder if fixed the cars?
Well, if you think about what it ought to mean,
it's a fine thought.
It means, for how many mechanics is it
the case that those mechanics fixed the cars?
But, while that's a fine thought,
you can't express it that way.
You can't express it by saying, how many mechanics
did he wonder if fixed the cars?
That's, for some reason, not one of the sentences spoken
in this speech community.
Nobody learned that.
Nobody was taught it.
It's not obvious what the reason is.
There's a lot of work on it, generalizes to other languages,
other constructions, and so on.
But it has a name.
It's called the empty category principle.
Don't forget that.
And there are interesting cases where it doesn't work.
And the cases where it doesn't work
have interesting explanations.
So that's-- linguists among you know about this.
Well, that's a typical example.
You have a perfectly fine thought,
but language design prevents you from expressing it
in the simplest, most direct way.
Some complicated circumlocution is necessary.
Essentially one of innumerable cases
that shows how language design appears
to be inconsistent with efficiency of communication.
There are many cases-- I'll mention some others-- where
efficiency of design and efficiency of communication
conflict.
And in every case that's known, efficiency of design
prevails, which has interesting consequences about architecture
of thought altogether.
Come back to that.
Well, that example happens to be misleading
because it's too complicated.
Same questions arise from the very first steps
that the infant takes, and indeed everything that follows.
Well, about 60 years ago, a couple
of people, a couple of students, in fact, graduate students
at Harvard-- I was one of them-- began to raise some questions
about this.
Take Bloomfield's definition-- the collection
of language, the collection of sentences,
that can be spoken in a speech community.
Well, if there's something to that,
there has to be a way to characterize that set.
Your brain is finite, so there has
to be some finite procedure, some computational procedure,
that determines that set.
The computational procedure has to determine,
for each language, an infinite collection
of structured expressions, each of which
has a dual interpretation.
It can be externalized in sound or in sign
or in touch or some other way.
And it can be interpreted by the other cognitive systems.
What's its meaning?
What does it entail?
And so on and so forth.
So it's what's called a generative procedure,
technically, a generative procedure that
yields an infinite hierarchy of structured expressions
with this dual interpretation.
That actually is a way of capturing a very
classical view, going back to Aristotle,
and a perfectly commonsense view, that language
is sound with a meaning.
We would nowadays say other modalities
with a meaning, like, say, sign with a meaning.
That sounds right.
I'll question it in a moment.
But this approach later has been called
the generative enterprise.
It took for granted that language does exist,
and furthermore that it's a biological object.
It's in your mind somehow.
This is somehow supposed to make that clearer.
The notion is called I language, internal language.
It's a concept of language that's
internal to your mind, particular individual,
actually.
The I also is a technical reference.
The account of it is what's called intensional,
intensional with an S. Means you care
about the actual mode of generation,
not just the class of expressions generated.
It's a crucial distinction.
So that's what it is.
That approach to language was later
given a name, which is now widely used,
by Massimo Piattelli, who's here.
Called it the biolinguistic approach, standard name.
Takes language to be a biological component
of the mind, like others-- in fact,
like other systems of the body, say,
the digestive system, or the immune system, visual system,
and so on.
This approach to the mind and the body
is called modular in biology.
That is, it treats a complex organism
as a collection of components-- sort of organs,
roughly speaking-- each of which has
some kind of internal integrity, and can be studied.
Its nature can be studied.
And it interacts with other subcomponents
in the life of the organism.
Doesn't mean that you can kind of cut it out.
Like, you can't cut the immune system out of the body.
But it is a system.
It's a module which has its own properties, interacts
with others.
Same, incidentally, with language.
I mean, nobody seriously expects that you're
going to find a piece of the brain which you can cut out
and that's language.
Different parts of the brain are involved
in language in different ways, but that's a different matter.
And of course, these systems have their own growth
mechanisms.
They grow in particular ways, develop in particular ways,
along their own lines, and they function in their own ways.
That's sometimes been called the norm in neuroscience.
Randy Gallistel, very fine neuroscientist,
points that out, and I think that's right.
There's no reason to believe that the human mind
and language depart from the biological norm
in this respect.
In fact, there's every reason to think that they don't.
Language turns out, when you really
look at it, when you look at the generative procedure that
has these consequences, seems to have very special properties,
which aren't found in other organisms,
or even in other subsystems of humans,
unless they're derivative from language.
But as far as we know, maybe there's some,
but none are known.
This doesn't seem to have anything homologous or even
analogous, in other organisms.
Furthermore, this system in our minds, the language system,
emerged millions of years after separation
from any other surviving creature.
There may have been others before,
other lines of hominids, but they're all gone.
Probably humans killed them.
But anyway, they're gone.
But among surviving animals, the closest related
are many millions of years separated from humans,
and many millions of years separated
from the origin of language.
So it wouldn't be too surprising if this is a very
special module of the mind.
And it's at the core of human activities, human creativity,
and so on.
Well, that opens the door to a discussion of evolution, which
is a very hot topic these days.
There's a huge literature on evolution
of language, books coming out all
the time, innumerable articles, conferences, et cetera, which
is kind of curious, because there
are much simpler questions which aren't studied,
barely studied at all.
So why is this one studied?
So take, for example, bees.
Bees have quite complicated communication systems.
Some of them do.
Some don't.
But there's very little literature
on evolution of the communication systems of bees.
And the reason is quite simple.
It's understood to be just much too hard.
So therefore, you don't study it.
Science studies the kinds of things
you can-- kind of like right at the edge of understanding.
Maybe you can make a little progress.
And the study of evolution of bees
is far simpler than the study of evolution of human language.
For one thing, there's about 500 species of bees,
so there's a lot of comparative evidence.
Some of them have quite complex communication systems,
and, in fact, different ones.
Some have no communication system at all that
anyone knows.
And they've seen them make out about as well
as the ones that do the waggle dance and all these things.
So raises a couple of questions about what
all this stuff is for.
But in any event, there's plenty of comparative evidence.
They have tiny brains, the brain about the size of a grass seed.
You can do any kind of experiment you like.
You don't need consent forms, nothing like that.
And they have a very short gestation period,
so you can breed them and so on.
So it's a perfect animal to study.
But take a look at the literature.
There's very little on evolution of communication systems
of bees.
On the other hand, humans, a huge literature, and a growing
literature, what's called evolution of language.
Another curious thing about it is
that the few serious evolutionary biologists who've
paid any attention to it are extremely skeptical.
The main one who's written about it
is Richard Lewontin, a very fine evolutionary biologist
at Harvard, who's written some of the main articles
on evolution of cognition.
And his conclusion is, go home.
You know, you're not going to learn anything about it.
He said none of the kinds of evidence
that evolutionary biologists know how to use
is available for the study of evolution of cognition
or particularly of language.
I think it's a pretty persuasive article,
but nobody seems to pay any attention to it.
Another reason why it's curious.
Another aspect of it that's curious
is that if you look at this huge literature,
turns out they're not studying evolution of language
at all, with very rare exceptions.
They're studying the evolution of communication.
That's a different topic.
Communication isn't language.
Now, there's a kind of a dogma that-- the phrase is,
"the function of language is communication,"
whatever that means.
Biological systems don't have any particular function.
They do lots of things.
But that's a dogma, so therefore people
study the evolution of communication,
or at least speculate about it.
They speculate about the evolution of communication.
And then there's a title that says, evolution of language,
or evolution of grammar, or something like that.
Well, if you study it, you can study evolution
of communication, of course.
At least you can speculate about it.
But it's kind of misleading, because in the case
of communication, there are analogs.
In fact, every organism we know of, down to bacteria,
has some kind of communication system.
So you can mislead yourself into believing
that, well, human communication just must
be kind of more of the same.
That's especially true if you are kind of entranced
by an old and long-forgotten idea,
or buried idea, called the great chain of being,
which was believed at one point, that there
are simple organisms like bacteria,
and then they get more and more complex,
and finally you get to us.
We're the top of the chain.
But in fact, you know, chimpanzees
have evolved as long as we have, and so have bees,
and so have dolphins.
There's no chain.
It's just lots of different organisms.
And in fact, when you take a look at this literature,
you notice something else that's curious.
There's never any characterization of language.
Now, if you're studying the evolution of the eye, which
does have a rich and significant literature,
it's never done by people who don't tell you what an eye is.
You have to know what an eye is if you want to-- at least
you have to know something about it,
if you're studying its evolution.
But if you take a look-- and you can't study the evolution
beyond what you know about the eye, plainly.
But if you take a look at the literature on evolution
of language, there's no characterization of language.
Well, if you're really studying communication,
maybe that doesn't matter.
But anyway, also the study of evolution of communication kind
of carries with it connotations that easily lend themselves
to serious misinterpretations of evolutionary theory.
I mean, there is a kind of an elementary school version
of evolutionary theory, which says that everything
happens by very small steps.
A little more of this, a little more of that, finally
you get complex things.
And there's certainly something to that.
But there's plenty in what's known about evolution that
doesn't work like that at all.
It's sudden changes, small, very small, reorganizations of, say,
the regulatory patterns of genes can lead to totally
different organisms and so on.
There's plenty of literature.
Well, just to illustrate, going back to that same Science
article a year ago, by a well-known scientist,
he's reviewing a set of books on evolution of language
and so on.
And he kind of dismisses them, because the ones he's studying
are all tainted by a false assumption, namely,
the assumption that language exists, that is, that there are
rule systems of some kind that determine the form
and meaning conditions, connections,
and the conditions of language use--
as, for example, in the one sentence that I gave you.
And he's already denied that.
He said it's just a collection of cognitive processes.
Well, the editors of Science, you
know, presumably serious scientists,
added a photograph to illustrate the point of the article.
The photograph is of three infants, multiracial,
you know, we're kind of very politically correct these days.
So three infants, and if you look at them,
they're kind of paying attention to each other.
And the title of it says, "communication without syntax."
So that shows that you can have communication without syntax,
and therefore, there's something wrong with the idea
that there are rule systems.
Actually, they could have had a picture of three bacteria.
It would have made exactly the same point.
They also communicate without syntax.
The title of the article is, "Without Social Context?"
with a question mark, meaning how can anybody study language
without looking at the social context of use?
And of course, study of the mechanisms of language
ignores the social context of use.
I mean, you can go on to study how,
once you have the mechanisms, they
are used in a social context, but that's different.
Well, if you're studying communication,
that makes sense.
So in fact, the study of communication
without social context would be almost meaningless.
So of course, social context is a crucial element
of the study of communication.
On the other hand, it's not an element
of the study of the mechanisms that
are used by the organism in vision or planning or anything
else.
So if you take the sort of standard work-- take,
say, the theory of vision, which is the most relevant to this,
closest, in some ways.
There's major work, classical work,
from half a century ago identifying the structures
in the striate cortex, the first part
of the brain that visual signals reach,
that kind of analyze the visual signal into straight lines,
pieces of straight lines, angles, and so on and so forth.
That's Hubel and Wiesel, and, you know,
Nobel Prize-winning work.
They didn't study the social context-- Another mic?
Let's see.
Let me try--
How's that?
Working?
I told you we had a former MIT student here.
[LAUGHTER]
I'll keep it as a spare.
I'll hold this.
We'll give him an extra PhD for this.
[LAUGHTER]
[APPLAUSE]
Well, I was talking about the way scientists
carry out the study of the mechanisms that
enter into behavior and action.
And in the study of those mechanisms,
they don't pay attention to social context.
So the Hubel and Wiesel cases are an example when, say,
Elizabeth Spelke and Renee Baillargeon and others
study the notions of how infants identify, discover,
internalize, or at least recognize
the concept of continuity of objects.
You know, an object moves behind a screen,
you expect to see the same object coming out.
That kind of thing.
They don't study the social context
in which that is carried out.
Just to mention one of the most remarkable discoveries,
I think, in the theory of perception,
there's a principle called the rigidity
principle of Stephen Ullmann.
He discovered that if a subject is
given a series of presentations-- very few,
maybe three presentations-- of a couple of points
of light on a screen, what the subject will see
is a rigid object in motion.
That's just the way we are designed,
our visual system is designed.
Well, obviously the social context doesn't matter.
Equally obviously, it's not learned.
And in fact, any work that is trying
to investigate the properties of the module, the ones that
are put to use in some way, doesn't pay attention
to social context.
That should be obvious.
Well, going back to the same Science article,
the author adds that there are well-developed gradualist
evolutionary arguments-- "gradualist" means
small steps-- gradualist evolutionary arguments
to support the conclusion that there's
no such thing as language, except as a complex
of independent cognitive processes.
And he also rejects, as do many, what
he calls the saltationist argument, that is,
the argument that there's a sudden gap,
you know, something happened not step by step.
The crucial question is how you get from finite to infinite.
And it's supposed to be a great heresy
to say that this is saltationist.
You have to do it step by step.
That's about as interesting as saying that 2 plus 2 equals 4.
It's a point of logic that it has to be saltationist.
But that conflicts with the dogma that everything
has to happen by small steps.
So throughout the literature, and even in this article
in Science, you got to reject it somehow,
strange confusions about evolutionary biology.
Well, do we actually know anything
about evolution of language?
Actually, there's some conditions,
there's some facts about it that we know.
Not much, but they are suggestive.
So one thing that we know with high confidence
is that for roughly 50,000 years,
there has been no evolution of the language faculty.
And the reason we can be pretty confident about that
is that it appears that our ancestors left Africa
roughly 50,000 years ago.
And if you look at all humans in the world,
they seem to have the identical capacity.
Now, there are individual differences.
But there's no known group differences.
So for example, if you take an infant
from a tribe in the Amazon that hasn't had human contacts
for tens of thousands of years, if the child's brought, say,
here, it will speak just like any kid here.
You know, and conversely, which indicates
that there just hasn't been any evolution
of this capacity at all.
Individual differences, of course,
always appear to some extent.
So that's one thing we can be pretty confident about.
Something else that we can be reasonably confident about
is that if you go, say, maybe 50,000 years before that,
there's no evidence that language existed at all.
Of course, we don't have tape recordings,
but we have an archaeological record.
And it appears that roughly in that very small window--
you can double the numbers and nothing changes.
It's a very small window from the point
of view-- from an evolutionary point of view.
There is a sudden, pretty sudden, explosion
of complex artifacts, many signs of innovation and creativity,
symbolic representation, representations
of astronomical events, complex social structures and so on.
One well-known paleoanthropologist,
Jared Diamond, calls it the Great Leap Forward.
And it seems to have happened very suddenly,
from an evolutionary point of view,
which means maybe over tens of thousands of years.
That's very suggestive about the nature of language.
It suggests that something happened
at that point which yielded this creative capacity that we
make use of in language.
Now, recall that at the core of language, every language,
is a generative procedure, which has the general properties
I mentioned.
It yields infinite array of structured expressions
with interpretations.
And if this emerged suddenly, there's
only one way known in which that could have happened.
There had to be some kind of rewiring of the brain,
maybe very small rewiring of the brain.
So little is understood about the brain, you don't know.
But some kind of rewiring of the brain
which provided this capacity.
Well, that can happen only after some mutation, maybe a mutation
in regulatory genes, something else to be discovered.
But whatever it was, that happened in a single person.
Groups don't undergo a mutation.
A person does.
So some person underwent this mutation, had a brain rewiring,
suddenly had this capacity.
And notice that that person had what's sometimes
called a language of thought, that
is, the structured expressions that
were made available unbounded could relate
to conceptual structures that already existed,
which means that to some extent, this person could think, plan,
interpret, and so on and so forth.
But it, let's call it, was the only person that could do it.
Nobody else could.
So a couple of consequences follow
from that, plausible ones.
One is that there were no selectional pressures at all.
So therefore, whatever developed in the brain was acted upon
solely by laws of nature and general principles that hold
no social context, obviously.
Well, what laws of nature?
It's a computational process, so at the very least,
there should have at least been principles
of computational efficiency that determined
what this thing is in the mind.
So it should have been kind of like a snowflake.
It comes out perfectly, simply because of whatever the laws
of nature are that apply to it.
So you'd expect it to be basically perfect.
Furthermore, it had no externalization.
Since this person is the only one who had that capacity,
there was no point whatsoever in figuring out
a way to get it to come out of the mouth
or out of the hands or anything else,
because nobody else has this.
So what you would have had is something
like an internal snowflake, connected to thought processes,
but with no externalization.
Well, speculating on, if this capacity had
some selectional advantage, it might
have proliferated to offspring.
Now, that's not so trivial, because it's well
known that small changes that have selectional advantage
almost always don't proliferate.
Sometimes they do, or we wouldn't have evolution,
but usually they don't.
So in fact, this might have happened many times
in the proto history of the species.
But we know it happened once, at least, because it remained.
Well, when it proliferates to descendants,
after a couple of generations, in a small hunter-gatherer
group, it could come to be a substantial part, maybe even
dominate the group.
At that point, there's a reason for externalizing,
for one person to have a means to convey
to others what it's thinking.
But that's a very tough cognitive problem.
You've got this snowflake in the mind.
You have a sensorimotor system which
has been around for hundreds of thousands of years.
For that there is fossil evidence.
And you have to somehow match these things.
Complicated thing to do.
Well, that accords pretty well with some general things
that we know about language.
Apparently, the overwhelming mass of complexity of language
is in the externalization.
And the variety of languages appears
to be mostly, perhaps entirely, in the externalization.
When you study a second language,
you study the externalization.
You're not taught anything about the examples
like the one I gave.
For one thing, nobody knows how to teach you that, because they
don't know how it works.
But you're not taught any of that.
When a child is acquiring a language,
it's fundamentally acquiring the externalization.
The externalization is easily susceptible to change,
like the Norman invasion changed the externalization
of English enormously.
But no indication it changed anything inside.
And that's roughly what we understand.
And that tells you what to look for,
gives you some suggestions about what to look for,
when you're asking what the nature of language is.
Well, let's go back to taking a look at that.
If you take the generative enterprise seriously,
you want to ask yourself, what is this procedure
that has these consequences?
It instantly turns out that there
are puzzles all over the place.
I mean, I gave one example.
But that's, again, misleading, because almost everything
looks like a puzzle.
Everything that was assumed to be simple and straightforward
in the 1950s, completely unproblematic,
turns out to be very puzzling.
That's reminiscent of things that have happened
in the history of science.
So if you take a look at the history of modern science,
for millennia, there were some things
that were assumed to be perfectly obvious
and not cause any puzzles, like, for example, the fact that,
say, if I hold this up and it happens to have boiling water
and I take the top off, if I take the top off and let
go of it, the steam will rise and the bottle will fall.
Well, there was an explanation of that that
goes back to classical Greece.
They're seeking their natural place.
So that solves that problem.
Galileo allowed himself to be puzzled about this.
He asked, well, why is this happening?
So did other scientists of his era.
That's the birth of modern science.
As soon as you allow yourself to be puzzled about things
that look obvious, it turns out at once you
don't know the answer.
And when you look a little farther,
it turns out that all your intuitions are wrong,
like about the rate of fall having
to do with mass and so on and so forth.
So that's where science begins.
It really begins with the willingness
to be puzzled about things that look obvious.
And that's very much what happened in the early '50s.
So just to give a couple of illustrations,
one hoary example has to do with a process called
auxiliary inversion.
An auxiliary verb, like "can," or "be,"
or something like that, moves from where it's interpreted
to the beginning of a sentence.
So you have sentences like, "Can eagles that fly swim?"
OK, simple sentence.
Well, we know what that means.
We know some puzzling things.
We know that "can" is connected to "swim," not connected
to "fly."
Similarly, if you put "are," "be," a form of "be"
in the front of the sentence, say, "Are eagles that fly
swimming?" that's OK.
But you can't say, "Are eagles that flying swim?"
Again, that's a perfectly fine thought.
It asks, is it the case that eagles that are flying swim?
But you can't say it as, "Are eagles that flying swim?"
You can only say, "Are eagles that fly swimming?"
very much like the "can" case, and in fact, every case
works like this.
And that's extremely puzzling, because the simplest
computational procedure would say
that the auxiliary verb in the front picks the closest verb
and attaches itself to that for its interpretation.
That's a trivial computation.
But it's not the one that's used.
The one that's used is much more complex.
It's called structure-dependent.
It requires that you know the structure of the sentence.
You find the main verb, not the closest verb,
which is a complex computational procedure.
And this is all languages, all constructions,
every place that the question can even be raised,
you have the same conclusion.
What the mind is doing is computing structure, not
linear order, which is very surprising.
It's a very puzzling fact, not trivial at all.
Well, that's incidentally one of the rare cases
where in computational cognitive science,
there have been concerted efforts
to try to show that statistical procedures can deal
with a real problem, this one, and there's
a big literature about this.
It's reviewed in print.
Actually Massimo and I have an article about it,
with Bob Berwick, and I won't run through it.
But if you look at it, you can see
that every effort not only fails, but fails
dramatically, like unresurrectable.
Furthermore, more fundamentally, and I
don't think we made this point clearly enough,
it wouldn't matter if the methods worked,
because if the language worked the other way, if you
used linear order, the same methods would give you
those results.
So even if they worked, they wouldn't
be telling you anything.
That's one of the very rare cases which has actually
been studied and doesn't work.
So what's the explanation?
Well, there is a far-reaching explanation that comes to mind.
The explanation would be pretty much
what I said about speculations about evolution.
Order is part of externalization.
The sensorimotor system requires that you have order,
or some other arrangement.
For example, sign doesn't use just linear order,
uses a lot of simultaneous things.
Depending on the modality, you might
have different arrangements.
But there has to be some arrangement
for the sensorimotor system.
On the other hand, there's no reason at all
to believe that for a semantic interpretation,
the order matters.
I'll talk about that in a minute.
But it is a reflex of the sensorimotor system.
So a plausible conclusion is that order just isn't there
when the computations are being carried out,
so therefore there's no choice of using order,
because it's not available to the computational system.
And there's actually some interesting neurolinguistic
evidence about this.
There's a group in Milan.
The linguist in the group, Andrea Moro, a lot of you know.
They did a study, relevant, looking
at this, basically, in which they gave subjects
two essentially nonsense languages, invented languages.
One of them was-- these are monolingual German speakers.
One of the languages was modeled on Italian,
so it satisfied the general principles of language.
The other violated what we take to be
general principles of language, sometimes called principles
of universal grammar.
For example, it used order.
Examples would be, say, making a negation-- you know,
the negation of a sentence-- forming it
by taking the third word of the--
by putting the negative element, "not",
in the position of the third word,
OK, which is not the way language works,
but it's a very simple way it could work.
Well, it turned out that the results
are that for the nonsense sentences that
are modeled on a real language, they
got the usual activity in the language-related parts
of the brain, Broca's area.
Just normal activation.
But for the cases that violated principles, like using order,
people could solve the problem, but they were apparently
solving it as a puzzle.
Other parts of the brain were doing it,
and took longer, and so on and so forth.
That's a very suggestive result. And more work of that kind
ought to be done.
Well, in any event, there is a far-reaching thesis
which is plausible, and it seems to be quite general.
So let me take just one second example.
This is one of the other few cases
where there has been some effort to pay
some attention to the real problem
in computational cognitive science.
Problem of language, that is.
Take questions of what's called binding theory, the theory
of referential dependence.
When does a word called an anaphor, like, say, "himself",
relate to an antecedent, like "he" in "He saw himself?"
Early approaches in this-- '50s and '60s,
'70s-- assumed that order was crucial.
And there's a lot of work showing
how this depends on the order of the elements, which
kind of looks right for the overwhelming majority
of sentences you think about.
But by the 1970s, it was beginning
to be understood that this was wrong.
So sometimes you get cases where the order doesn't work.
And so take things like "He thinks that John is a genius."
"He" doesn't refer to John in that case.
Refers to somebody else.
Take "His mother thinks that John is a genius."
There "he," "his," can refer to John.
But the order is the same.
So it can't be just order.
And in fact, if you look at this,
it depends on hierarchy, not order.
The "he" is buried inside "his mother,"
so there's a hierarchic structure that differs.
Furthermore, you can violate order, or at least
superficially, so it appears.
So take, for example, "Which picture of himself
does John like best?"
Well, there, the "himself" goes back to John.
But it precedes John, at least in the external order.
Well, that kind of research led to the conclusion
that in fact, order doesn't play any role at all,
that it's just hierarchy.
And that appears to work for many other cases, every known
case, in fact, of semantic interpretation.
This, as I said, is one of the rare cases where
there has been an attempt to approach the problem
from the nonexistence approach.
There's a recent issue of the same journal, Cognitive
Science, by two well-known scientists
who argue that the antecedent-anaphor relation is
just a simple cognitive process.
The, say, John-himself relation is
resolved as quickly as possible in parsing and interpreting
the sentence.
You just solve it as quickly as you can.
Well, that's actually, clearly-- you can easily
show that that's false.
Even the examples I mentioned show it.
But take a more interesting one.
Take the sentence, "Do they expect
to see each other next week?"
OK, there, "each other" goes back to "they."
Now add the word "who" in front.
"Who do they expect to see each other next week?"
Doesn't work anymore. "Each other" doesn't go back
to "they."
It's exactly the same sentence, just with the word
"who" in front.
Well, intuitively we know the reason.
Intuitively, we know that although you
hear the "who" in front, it's actually being interpreted
internally to the sentence.
It's being interpreted as something
like, "For which person do they think that that person,
they think that they expect that person to see each other?"
Well, then you can't do it, because there's
a "that person" in the way.
Well, you don't hear that.
But evidently the mind is hearing it.
So the mind is picking up that missing element,
and it's interpreting it even though it's not there
in the sound.
Incidentally, this bears directly-- one of many things
that bear directly-- on the kind of dogma
that nothing can be going on in the mental processing that
isn't accessible to consciousness,
because you can't possibly introspect about this
and figure out what's going on.
Well, it also shows that if you want to look into it,
you quickly get into pretty complex questions about how
the generative rules, the syntactic rules,
operate, the things that the nonexistence approach is trying
to replace.
This becomes much more obvious if you look at-- this goes on
to much more complex examples.
Well, let's turn briefly to the generative process.
Now, there are three kinds of questions
that you can ask about it, as in any scientific investigation
of any other biological object.
Now you can ask, first of all, "what" questions.
What is it?
What we might call "how" questions, how does it develop?
And "why" questions, why is it this way
and not some other way?
Those are the most interesting.
So take this case.
The "what" questions, the natural approach
is to start with the very simplest assumption
and see how far it can take us.
So what's the simplest assumption?
We have a computational procedure.
Every computational procedure contains within it,
somewhere buried within it, a very simple operation that
says, take two objects already formed,
put them together, make a new object.
OK, call that merge.
That's going to be anywhere in a computational procedure.
If this is as simple as possible,
the two objects that are brought together
are not changed in the course of the operation.
If they are changed, that's more complex.
So the simplest assumption is, well, they're not changed.
Furthermore, they're unordered.
To order them is more complex than to leave them unordered.
Well, what that means is that the simplest computational
operation takes x and y and just forms the set containing xy.
Nothing else.
So let's take that to be merge.
Well, if you think about merge, it
has two logical possibilities.
One of them-- about x and y.
One possibility is that x and y are distinct from one another,
and the other possibility is that one of them, let's say y,
is inside x.
And that exhausts the possibilities.
It turns out there's no possibility of overlap.
Well, let's call those external merge and internal merge.
So for example, if you say "They see each other,"
the pieces are put together by external merge.
Now, let's go back to "Who do they expect to see each other?"
What's going on there, and would account
for the facts, is that the "who" is actually
taken from the sentence, "They expect who to see each other?"
It's part of that sentence.
And it's merged to it.
So you get, "Who do they expect to see each other?"
Well, if nothing is changed in the process,
that's internal merge.
If nothing has changed in the processes of computation,
you have two copies of that "who."
So what the mind is really hearing
is, "Who do they expect who to see each other?"
And then the mind just uses the obvious operation, looks
for the closest antecedent.
Of course, the ear isn't hearing that.
The ear is hearing something else,
because in the course of externalization,
the internal one is deleted.
Now, why should that happen?
Well, that's another obvious principle
of computational complexity.
The less you have to articulate, and the less phonology
you have to run through, the simpler it is.
So the optimal procedure is to just erase everything
in externalization, except the one thing that you need,
the hierarchically highest, to show
that the operation took place.
Now, that happens universally, every language,
every relevant expression.
It has an interesting consequence.
That yields difficulties for perception.
People who've worked on parsing programs,
ways of figuring out what a sentence means,
know that one of the hardest problems
is what's called the filler gap problem.
You hear a word like "who" at the beginning of a sentence,
and you've got to figure out where you're interpreting it.
That's pretty tricky.
If you repeated it, you'd have essentially no problem at all.
In complex sentences, this can be quite complex.
Well, here's a case in point, an important case,
where considerations of computational complexity,
making the system as simple as possible,
conflict with efficiency of use.
There's a conflict between computational efficiency
and communicative efficiency.
And computational efficiency just wins, hands down,
all the time.
And there are many other cases like that.
I won't go into them.
And they all seem to have the same consequence,
insofar as we understand it.
And that strongly suggests that the Aristotelian maxim that
I mentioned, language is sound with a meaning,
ought to be to be revised.
It should be, language is meaning
with some kind of externalization, which
is a very different conception.
It means that language is essentially a thought system.
And then, on the side, there's some ways
of externalizing it, one or another.
That looks like the correct answer.
Well, let's quickly turn to the "how" questions, language
acquisition.
I won't go into detail, but clearly that
can take place to the extent that you have some answer
to the "what" questions.
If you know what you're aiming at,
you can ask the question, how it's acquired.
Otherwise not.
Then come the "why" questions, the hard ones.
Why would language have these properties, rather than
some other properties?
And the best answer would be pretty much
what I speculated about.
Some sudden, very slight event yielded
merge in its optimal form, the form that's least complex.
And the rest follows from natural law.
So, kind of things I've talked about.
Well, we're very far from reaching that goal,
but it looks a lot more plausible
than it did not many years ago.
And it is the goal that is the natural kind of point
to which the study of the topic ought to strive.
It has a name these days.
It's called the minimalist program.
And that's led to a lot of misunderstanding.
It's just normal science, you know,
there's nothing to say about it.
It's the effort to try to show that the simplest
assumptions carry you.
See how far they can carry you.
Maybe all the way.
Maybe a little, some residue.
But that this should be true is even kind of suggested
by the very little we know about evolutionary history.
Well, before I end, let me just mention
one more extremely serious problem which
has barely been addressed.
Any computational procedure assumes
kind of atoms of computation, the smallest
things that you compute with.
So minimal meaning-bearing elements, kind of roughly,
words.
Let's take words as a first approximation.
Well, there's two aspects to this.
One has to do with sound, the other with meaning.
The question is, how do these things relate
to the mind-external world?
Well, in the case of externalization,
there are fields that study it.
Acoustic and articulatory phonetics
are effectively the study of how these units, their elements,
relate to the mind-external world.
That's a very non-trivial investigation.
It's been going on seriously for 60
years with high-tech equipment and so on.
Some results, but hard problem, like most scientific problems.
Well, what about the other aspect?
How do these minimal elements relate
to meaning, something outside, and to use of language
and so on?
Well, there is a standard answer to this.
It's called the referentialist doctrine.
It holds that a particular word, let's say the word "cow,"
picks out cows, those things out there,
things that some physicist could identify, maybe
by some kind of causal relation.
That's virtually dogma in modern psychology and philosophy.
It seems that it may be true for animal communication.
So as far as we know, animal communication,
the symbols in animal communication,
do apparently pick out mind-independent entities
that a external observer could identify.
Like, if a monkey has a certain call,
it could be elicited by, say, leaves
moving around in the tree.
We interpret that as a warning call.
An eagle's coming or something.
And apparently, animal communication
is like that everywhere.
So it does seem to satisfy something
like the referentialist doctrine.
However, nothing like that is remotely true for language,
even for the simplest elements.
Take "person," "river," "tree," "it," "thing."
Anything you pick, as soon as you look at the meaning,
you find that that's just not true.
You cannot individuate the entities that people use
the term to refer to by any mind-independent properties.
So, a lot of work on this.
I won't talk about it.
Now, the most you can say is that these internal things
provide the kind of perspectives from which one
can-- that you can use for interpreting and referring
to the world.
Actually, that was kind of understood
in the 17th and 18th centuries.
There was a lot of work about it then,
discussions, trying to argue that these elementary things
involve what we now call gestalt properties,
cause and effect, sympathy of parts, things
directed to a common end, psychic continuity,
crucial for individuating animals and people and so on.
Most of this has been forgotten, but there's pretty good
evidence that it's true.
And if it is, it's another vast chasm
between humans and other animals,
and a huge problem for evolutionary biology.
The origins are completely unknown.
And if Dick Lewontin is right, maybe never will be.
Well, one final comment, just for a few of you,
those who are familiar with all this stuff.
There are some, and I apologize to the rest.
This will only be a minute or two.
Now, there are some important differences
between early generative grammar and the kind
of merge-based systems that I've been talking about here.
The basic properties that were studied from the beginning,
we can divide into contiguous and noncontiguous relations.
So you go back to early transformational grammar,
generative grammar, there are contiguous relations
like composition, order, and projection.
And those were handled in phrase structure grammar.
Then there are noncontiguous relations.
The crucial one is displacement, long-distance agreement.
These were handled by transformational grammar.
That was the division of labor.
By the 1960s, changes were coming along.
X-bar theory developed, and it overcame a lot of problems.
So phrase structure grammar is highly stipulative
and extremely complex.
It's kind of strange it's still used in computational cognitive
science.
It's the worst possible system.
There's all kind of things wrong with it,
which were known 50 years ago.
X-bar theory overcomes these things,
but it introduces some other stipulations, which
haven't been even noticed, let alone given enough attention.
One thing, X-bar theory requires that all constructions
be endocentric.
Another consequence is that you get things like first merge,
second merge, and so on.
You get specifiers and multiple specifiers.
Now, all of this is highly stipulative.
And you can see it even if you look at the simplest rules.
Like, take the standard first rule
of old-fashioned generative grammar, [? SRONPVP. ?]
We put it a little differently now, but the same issue arises.
Now, what about the NP and the VP, the subject
and the predicate?
Well, in an X-bar theory, one of them
has to be the specifier or the other.
And the structure has to be endocentric.
There has to be a head.
What's usually assumed is that the T,
the inflectional element, is the head,
and the subject is the specifier.
But that's completely arbitrary.
You could just as well say that the TP
is the specifier of the NP, of the N, the head of the NP.
And this runs all through the system.
Furthermore, there's no reason to believe
that all merges is to a head, which would
make everything endocentric.
That's an extra complication.
And these things have to be looked at.
That's another puzzle that you can't just ignore,
although it's taken for granted in almost all work.
Well, if you look at a merge-based system
and you have no special constraints on it,
the division of labor is going to be different.
Composition and displacement will all be simple merge.
Order is taken out of the whole system.
It's externalization.
It reflects the sensorimotor system.
Now, what about projection?
Well, that's different from the other properties.
The other properties are virtually visible.
You see them in the expression.
Projection is a theory-internal notion.
You don't see it at all.
It's just somehow internal to the theory, which raises
the question whether it exists.
And if it does exist, what is it?
Well, there is a motivation for assuming
that something like it exists.
Projection tells you what kind of a phrase you have.
How could you tell that?
Well, the best way to do it would be by a minimal search
procedure, which goes from, I would say at least phase
head, to a designated element that tells you what kind
of a structure the thing is.
That works fine for endocentric constructions,
for something of the form, head-complement.
Then the head is picked up by minimal search.
But it doesn't work at all for other constructions,
[? XPVPYP ?] constructions, like, say, subject-predicate.
And there's no reason to rule those out.
Well, how can you find-- now it's
called labeling instead of projection, because you're
searching for something and trying to label it.
How can you overcome those cases?
Actually, there's only two ways to do it,
if you think about the logic.
One of them is an adaptation of some ideas
of Andrea Moro, again, what he called dynamic antisymmetry.
This is in his study of copular constructions,
where he argued that there's a [? copulant ?] of small clause,
and that one or the other of the two
elements of the small clause have to raise.
And he talks about the consequences of this.
You can reinterpret this as a labeling problem.
That is, unless you raise one of them,
there's no way of labeling the small clause.
If you do raise one of them, then only the unraised one
is visible.
The other one is there.
It's a copy.
But it's part of a discontinuous element.
And there's independent evidence that the search procedures,
like agree, don't look at the copies.
In fact, they only look at the whole discontinuous element.
That's the way the standard, say, intervention effects
in Icelandic work, and so on.
So that's one way of doing it.
If you get something like this, you raise one of them.
You can label it.
There's only one other approach.
And that is that if the two elements, the XP and the YP,
have essentially the same designated
element, the same head, then the search procedure
will find the same thing in both of them
and it will use that as the label.
Well, that gives you-- if you think it through,
it's complicated.
It gives you what Luigi Rizzi called criterial positions.
He kind of just stipulated the criterial positions.
But if you look at them, those are
cases where the two things that are put together, say,
a subject and a predicate, have agreeing heads,
like [? phi ?] features.
So those are the two cases.
And it seems that that's the way things work.
And in fact, it yields a lot of consequences.
For one thing, it explains why you
have to have successive cyclic movement.
In successive cyclic movement, you're
getting to intermediate structures, XP YP,
which are unlabelable, so you got to raise one of them
until it reaches a criterial position, in which they're
labeled the same way.
So like a WH phrase and a Q and a C and so on.
Well, there's a lot more to say about that.
But I'll stop there.
But it opens up a whole array of consequences,
of new puzzles that haven't really been thought about.
[APPLAUSE]
Dr. Chomsky, where do you see exclusive externalization,
like in Pig Latin, or changing language intentionally
to keep secrets, of causing the most
impact on the evolution of language,
before or after the 50,000 mark?
So how does evolution affect the externalization process
in the last 50,000 years?
As far as we know, it doesn't.
Externalization changes all the time.
It changes very rapidly in the history.
That's historical linguistics.
Historical linguistics is the study
of how existing languages have changed through time.
But if you take a look at it, it's
almost entirely the study of externalization change.
There is work on syntactic change,
but very possibly, if that's reanalyzed properly,
it may turn out that there isn't any syntactic change,
that the things that are changing
are either in the morphology or in the phonology.
And morphology and phonology are part of the externalization
process.
So I wouldn't be surprised if that turns out.
It's a big job.
It's a goal.
The goal would be to show that nothing changes,
except the externalization.
We do know that the externalization changes
enormously.
Now, that's why you have the Tower of Babel,
people can't understand each other.
Whatever came out of Africa 50,000 years ago
has gone in a lot of different directions.
And that's not too surprising, because every child
has to solve a very hard cognitive problem.
First of all, the child isn't presented
with a single language.
Never.
Even in the smallest community, there's
always differences in the way people talk.
Your aunt talks this way, and your friends
talk a different way, and so on.
So the blooming, buzzing confusion is pretty confusing.
And out of this, the child has to get from those noises
or signs or whatever they are to something inside, which may
be something like a snowflake.
That is, something which is just designed perfectly
by the laws of nature, maybe laws
of computational complexity.
Now, we don't know that.
It's just that the kind of evidence
begins to point to that.
But the effect, the result, is that as far as we can detect,
there is no evolution.
Notice that historical change is not evolution.
Evolution means genetic change.
OK, and there's no reason to think
that, say, tribes in Papua New Guinea
or whatever have a different language capacity than you
and I do.
Again, there's individual differences,
but that's a different matter.
We're talking about group differences.
I should say there is some slight evidence, very slight,
for group differences.
There's some work claiming-- I don't know how credibly,
there's some papers in the literature, maybe some of you
know something about this-- that speakers of tone languages
hear things a little differently from birth
than speakers of non-tone languages.
Is that right?
Yeah.
So you can ask Heidi.
She'll explain it to you.
[LAUGHTER]
Professor Chomsky, thank you for coming here,
and thank you for all your work in many facets.
On behalf of a group called Audit Arizona,
I'd like to just bring in some language with social context,
and present you with this packet, with your permission,
about a local lawsuit that's going forward
prosecuting the Pima County for election fraud.
Well, that's different.
[LAUGHTER]
OK.
Happy to have it.
Professor Chomsky, my question is on computing.
You know, there are a number of us
here on campus, presumably in this room,
who work on a field, so-called computational linguistics,
a field heavily influenced by your work.
So for obvious reasons, we grew up
being quite used to highly-structured views
of language.
You know, for obvious reasons, right?
So on the other hand, one trend that a lot of us
have been observing over the years is that in the field,
most successful computational applications
related to languages are built on relatively shallow,
you know, if any, shallow, flat, linear,
or almost no linguistic structure, motivated
computational models.
So two questions naturally arise in my mind.
One is what is your view on this?
What do you think is going on?
Is it a major departure from your framework, or just
a minor digression?
Or is it totally irrelevant question, how we build things
is totally independent of how we understand language?
So that's one question, what is going on?
But second question is, pragmatically,
do you see this shallow linguistic,
although highly-statistical computational approach
as a viable approach to building computational intelligence that
have language capabilities?
I apologize.
I don't hear very well.
In general, I think we should have one question per customer.
[LAUGHTER]
There's a lot of very good work in computational linguistics.
I mean, I think the things that I was discussing
are mistakes, and pretty prevalent,
but there's very good work right here.
[INAUDIBLE], for example.
Maryland has a very good program.
University of Michigan.
Rick Lewis does very good work.
At Penn, Charles Yang has done quite good work,
I think very good work, integrating
learning-theoretic ideas, a lot of them involving statistics,
with principles of language, and showing
that if you integrate these things,
you can account for some interesting phenomena.
Actually, in this MOM case, Yang is
one of those who showed that if you study
the statistical analysis, study of transitional probabilities,
try to get word boundaries out of that.
And he showed that doesn't work.
But he showed that if you take that information
and you also look at prosodic information,
you know, principles that say a word has a prosodic peak,
then you get better results.
And then there's further work by Shukla, Aslin,
and somebody else whose name I forgot,
which shows that if you look at prosodic phrases-- it's
a more complex notion-- and integrate
that with statistical analysis, you get even better results.
And in fact, I think Yang's most interesting result, at least
for me, is-- there's been a ton of studies,
you know, especially in the connectionist literature,
of how irregular verbs are learned.
It's kind of an odd topic.
Nobody ever really studied it.
But that was kind of like the heart
of the connectionist literature.
As far as I know, Yang is about the only one
who has a good explanation.
It's based on a proposal of Morris Halle's, actually,
that verbs, the irregular verbs, are first
put into classes, you know, like there's
the sing-sang-sung class and other classes.
And then he argues the probability of occurrence
does play a role in determining acquisition.
But rather strikingly, improbable verbs,
which are in a class that has high-probability verbs, those
are learned just as fast as the high-probability verbs.
So what's being learned, really, is the rules, not the examples.
Well, that's interesting work.
And there's other work of this kind which
I think makes perfect sense.
So first off, I'd like to think Professor Chomsky for coming
out and giving this talk today.
It was great.
So I'm not a linguist.
So excuse the potential naivety of my question.
But I am a thinker, and I've also often wondered
how things such as the establishment
of national languages or dictionaries
that limit what is a word or what isn't a word
have affected language.
So my question is, do you think that the state or other
organizations have hindered the evolution, the free evolution,
of language?
It doesn't affect the evolution of language.
Remember, evolution means genetic change.
So after you've been through a boring grammar course
in eighth grade, you may try to speak differently,
but your genes haven't changed.
So there's no indication that any of this
has anything to do with evolution.
Prescriptive grammar, there's nothing wrong with it.
Like, I use dictionaries, you know?
But remember, dictionaries, if they're any good,
they're giving you hints, and they're telling you
things that are idiosyncratic, that you couldn't know just
by being a human being.
So you couldn't know which meaning is pronounced
which way, or things like that.
So the Oxford English Dictionary,
you know, this huge thing, it'll give you information like that.
But most of the information you're providing yourself.
The dictionary doesn't come close to describing
either the meaning or the pronunciation of words.
You're adding all of that yourself,
just by being a human being, the same way
a child does when the child is hearing a word used
and somehow understands what it means.
And in fact, it's pretty remarkable,
but there's quite good evidence for one-trial learning.
And when you think about the meaning of a word,
that's kind of a miracle.
Lila Gleitman has done very good work on this, as have others.
A kid hears a word in one use, and it knows the meaning,
and how to pronounce it.
Maybe it can't pronounce it, because it
doesn't have the articulatory capacity,
but it knows how it's done.
In fact, a kid can learn a dozen words a day.
It is kind of miraculous.
And they do have complex meanings.
So the prescriptive grammars, at best,
just give you data that you couldn't
have known just by being human.
At worst, what they do, and what instruction often does,
is teach you things that are wrong.
Anything you learned about your language in school
is probably wrong, or else they-- for good reasons.
Otherwise they wouldn't have taught it.
Most of it you know automatically.
And if you're taught something, it's
typically trying to get you to do something that's
inconsistent with what you know, and very often inconsistent
with the way language works.
And this does have effects.
And that's why people say things like, "between you and I,"
let's say.
It ought to be "between you and me."
Or why they say "he and I left," instead
of "him and me left," which is what English ought to have.
That is a result of teaching.
And the teaching is based on misunderstandings about Latin
that go back a couple hundred years
and worked their way into teaching curricula.
And the teachers have to try to force
the kids to say these things, because they
don't make any sense.
And when kids learn them, they often overgeneralize,
so you get the "between you and I" type stuff.
So that's there, but it's pretty peripheral to language.
Very simple question.
When you refer to complexity, what
do we actually mean by that?
Complex as a dynamic system, or a non-linear--
Well, complexity is a tricky concept.
We don't have a general theory of complexity.
And in fact, the study of systems and how they develop
teaches you something about complexity,
because there's good reason to think
that systems kind of automatically
develop in the least complex ways.
When you see how they develop, you
get some sense of what the proper notion of complexity is.
So it's an interactive process.
But there are some things that are just kind of obvious.
For example, less is better than more.
Or, a shorter search is better than longer search.
And those are the only notions of complexity
that I used in the talk, and they carry you pretty far.
Nothing like this gets into dynamical systems.
That raises further questions, which may be relevant,
but you kind of wait until you reach problems where
you need more complex notions.
Sorry, but we have to stop the questions now.
[LAUGHTER]
I think I have no choice.
[APPLAUSE]
Thank you, Dr. Chomsky.
Thank you for being here.
Thank you for listening.
My question is probably non-academic,
in that language often falls short for many of us,
and we sometimes refer to foreign language
to try to explain something.
Why is it that my thinking process,
or my experiential process, cannot be articulated with
language?
Where is that disconnect?
Well, you know, each language-- first of all,
I think altogether, I think all of us
know that language does not give us the full capacity, maybe
anywhere near the capacity, to express what we're thinking,
feeling, hoping for, whatever.
There's an awful lot of thought that goes on that just
doesn't come out the mouth, you know, and probably can't.
I mean, when you introspect, you can see this.
So for example, here's a topic that hasn't been studied,
but should.
People are talking to themselves all the time.
You can't prevent yourself from talking to yourself.
Actually, I don't know if that's true of signers.
Do signers sign to themselves all the time?
All the time?
OK.
But you've seen it.
Well, you just can't prevent yourself from doing it.
It takes a tremendous act of will.
Now, if you pay attention, at least
when I pay attention, to what I think I'm doing,
it is externalized language.
Like, I can tell whether two words rhyme.
But I think it's fragments.
I mean, at least what I think I'm doing,
is sort of fragments come to mind, a word, another word,
and then all of a sudden, I know exactly what the thought is,
and I can express it.
Well, if that's true, that means that the thinking is going on,
the bits of externalized language
are kind of spurring you to express it,
and then you can express whatever it is you're thinking.
But even with all of that, there's a lot of stuff
you just can't express.
That's why people write poems and that's
why we have literature and art and music
and all sorts of other things.
And that tells us something, but not enough.
As for looking at other languages, I mean,
there are, sometimes, concepts, that
do have a-- kind of like the mot juste, the right word.
It happens like that, happens to be in another language.
That doesn't happen to be an expression of English.
And there are things like that, you know, schadenfreude, angst,
and stuff like that.
So there are cases where some other language just
happens to have the concept that you're looking for.
But there are plenty of cases where, at least,
my feeling is, where no language does.
And we just can't begin to exhaust
the content of our thoughts, feelings, emotions, and so on.
I think everybody knows this from their own experience.
It's not studied much.
Now, one of the reasons it's not studied
is because of the pervasiveness of certain dogmas,
like the kinds I mentioned, that nothing can be going on
unless you're conscious of it, or at least
you can become conscious of it.
That's a very deep-seated conviction
in philosophy and psychology.
I mean, I've looked-- maybe some of you know better--
but I can find almost nothing in the history of thought
that raised the question that there are unconscious thoughts.
And incidentally, that includes Freud.
I mean, there's a debate about this.
But as far as I read Freud, I think
when he talks about the unconscious,
I don't think he means by it, inaccessible to consciousness.
In fact, the whole technique of psychoanalysis
wouldn't make sense on that assumption.
So it's as if it's there somehow,
and you can bring it to consciousness.
But the idea that there's something--
that the stuff going on that is completely unconscious,
that's rarely discussed.
There's some in Jung.
There's some in [? Vico, ?] but not much.
And I suspect that this bears on current work that's being done.
Like, there was some very interesting work
that came out recently.
Some of you remind me.
I forgot the name of the experimenter, which was all
publicized all over the place.
Someone found that before you make a decision, let's say,
to lift your hand, there's something
going on in the motor cortex, that would be connected
with lifting your hand.
I forget who the guy was who did this.
This was interpreted all over the place
as being an argument against freedom of will.
It isn't that at all.
All it's saying is that decisions are unconscious.
So you make a decision.
Before it becomes conscious, already something's going on.
So all the problems remain exactly as they were.
But you learn something you should have guessed anyhow,
that things are happening that just aren't
accessible to consciousness.
Why should they be, any more than the rigidity principle is?
[APPLAUSE]
19
23
25
29
30
32
35
38
42
45
48
54
57
62
66
69
74
78
81
87
89
94
97
99
103
106
111
115
119
121
127
133
135
138
142
145
149
156
158
161
164
167
172
176
180
183
188
191
193
196
198
200
204
207
209
212
217
219
223
229
231
235
237
239
242
246
248
251
253
257
259
260
262
265
268
270
272
274
277
284
290
291
294
296
298
302
307
310
314
316
324
328
335
342
347
352
353
358
359
362
364
367
372
374
377
380
383
385
389
392
396
400
403
407
413
416
419
422
458
464
467
471
474
477
481
484
489
491
497
499
504
509
511
514
518
523
525
530
532
534
538
543
546
549
551
554
555
557
559
564
565
567
573
576
582
586
589
592
595
598
601
603
606
608
610
613
617
620
625
629
637
639
646
649
652
655
657
662
668
671
673
675
677
681
684
689
691
694
698
700
703
706
709
716
718
722
724
729
732
737
741
744
749
751
753
756
759
762
766
771
773
778
780
783
786
792
794
798
802
806
809
812
816
819
823
825
829
834
839
842
846
852
857
860
863
865
868
872
873
877
881
884
885
889
892
896
900
905
909
912
916
918
920
922
923
926
928
931
935
939
943
946
949
951
953
960
962
964
966
970
973
979
981
985
987
989
990
993
997
1002
1004
1006
1009
1014
1017
1018
1022
1024
1030
1033
1034
1037
1040
1041
1043
1045
1047
1049
1051
1055
1060
1062
1063
1065
1069
1071
1075
1078
1085
1087
1089
1093
1096
1098
1100
1103
1106
1107
1111
1115
1117
1119
1122
1124
1127
1129
1132
1134
1139
1141
1144
1148
1150
1153
1155
1158
1161
1165
1169
1171
1175
1177
1180
1182
1186
1190
1195
1197
1199
1203
1209
1211
1216
1218
1220
1223
1226
1229
1232
1236
1241
1242
1245
1251
1252
1257
1260
1262
1269
1271
1273
1277
1283
1287
1291
1293
1295
1299
1302
1305
1310
1314
1317
1320
1322
1325
1329
1331
1334
1336
1341
1345
1349
1354
1358
1361
1363
1366
1369
1374
1377
1378
1379
1381
1383
1385
1388
1391
1394
1394
1396
1397
1402
1404
1406
1410
1414
1415
1417
1421
1424
1426
1429
1430
1434
1435
1439
1441
1443
1445
1450
1453
1454
1456
1460
1464
1466
1471
1475
1478
1481
1484
1490
1494
1498
1499
1502
1507
1509
1510
1513
1516
1519
1524
1528
1531
1535
1536
1539
1542
1544
1547
1551
1553
1557
1560
1563
1567
1571
1574
1580
1583
1588
1590
1592
1594
1598
1602
1605
1608
1613
1617
1620
1623
1626
1629
1630
1632
1636
1638
1641
1644
1646
1649
1653
1655
1658
1659
1663
1666
1668
1671
1673
1678
1680
1683
1686
1689
1692
1697
1700
1702
1707
1708
1710
1715
1718
1724
1726
1730
1732
1736
1738
1740
1744
1745
1748
1749
1751
1753
1756
1759
1762
1767
1771
1772
1775
1778
1786
1791
1794
1799
1803
1804
1808
1810
1813
1819
1824
1827
1830
1832
1835
1837
1839
1844
1848
1851
1853
1856
1858
1860
1865
1870
1872
1874
1876
1879
1883
1885
1892
1894
1899
1901
1907
1909
1911
1913
1915
1918
1921
1921
1923
1925
1929
1931
1935
1936
1938
1942
1944
1946
1950
1955
1958
1961
1963
1966
1968
1970
1973
1976
1977
1983
1988
1990
1994
1997
1999
2001
2003
2005
2007
2011
2016
2018
2024
2027
2030
2034
2038
2040
2045
2047
2049
2052
2056
2061
2064
2067
2070
2072
2075
2077
2080
2083
2085
2087
2091
2093
2095
2098
2100
2103
2105
2108
2111
2114
2116
2119
2122
2124
2127
2130
2134
2136
2140
2143
2144
2148
2152
2156
2158
2160
2162
2163
2165
2167
2171
2173
2174
2179
2182
2184
2187
2190
2195
2200
2203
2205
2207
2210
2213
2215
2219
2221
2223
2231
2239
2243
2246
2250
2252
2254
2256
2258
2262
2264
2270
2275
2277
2279
2282
2288
2293
2293
2297
2301
2306
2310
2313
2316
2319
2320
2324
2327
2330
2336
2344
2348
2351
2354
2359
2364
2366
2369
2372
2374
2378
2383
2386
2389
2393
2395
2397
2399
2402
2406
2407
2410
2413
2417
2420
2422
2424
2428
2429
2431
2436
2437
2440
2448
2451
2454
2459
2461
2465
2467
2473
2474
2475
2477
2479
2483
2487
2489
2493
2495
2497
2505
2510
2513
2516
2517
2520
2526
2529
2535
2538
2542
2544
2546
2547
2549
2551
2556
2559
2562
2565
2570
2574
2577
2581
2583
2585
2587
2591
2594
2596
2603
2606
2607
2609
2613
2618
2621
2625
2627
2631
2634
2637
2641
2643
2647
2651
2654
2656
2658
2663
2666
2670
2672
2675
2678
2682
2683
2685
2687
2690
2693
2697
2700
2703
2706
2711
2714
2717
2719
2722
2724
2726
2730
2734
2738
2742
2744
2746
2748
2751
2754
2759
2763
2766
2768
2771
2775
2778
2780
2783
2787
2794
2798
2803
2807
2811
2815
2816
2821
2824
2826
2833
2835
2840
2844
2845
2848
2853
2857
2861
2865
2867
2870
2873
2876
2881
2887
2891
2894
2895
2902
2904
2907
2910
2913
2919
2923
2926
2929
2934
2936
2937
2939
2943
2949
2956
2958
2959
2964
2966
2970
2975
2978
2982
2985
2988
2992
2996
2999
3001
3003
3006
3008
3013
3015
3021
3024
3026
3029
3034
3036
3040
3042
3045
3049
3053
3056
3061
3065
3067
3071
3075
3077
3081
3084
3085
3088
3091
3094
3096
3099
3102
3106
3109
3111
3115
3117
3120
3122
3124
3127
3128
3130
3132
3136
3140
3143
3145
3148
3153
3156
3159
3162
3165
3168
3172
3175
3177
3180
3181
3184
3185
3188
3191
3194
3198
3200
3203
3206
3209
3213
3218
3221
3226
3229
3231
3234
3237
3240
3242
3245
3247
3250
3253
3254
3256
3258
3260
3263
3264
3267
3270
3273
3276
3280
3282
3285
3288
3290
3295
3298
3300
3302
3306
3308
3312
3316
3320
3324
3327
3332
3337
3340
3343
3344
3348
3351
3356
3360
3363
3365
3367
3369
3372
3375
3380
3385
3388
3392
3396
3399
3403
3406
3409
3411
3415
3417
3419
3421
3425
3428
3429
3434
3437
3440
3442
3445
3449
3451
3452
3454
3456
3460
3463
3465
3471
3473
3479
3482
3486
3487
3491
3493
3495
3496
3498
3500
3502
3505
3506
3508
3512
3518
3521
3525
3528
3535
3536
3538
3542
3546
3550
3555
3559
3562
3566
3570
3573
3575
3579
3583
3585
3587
3590
3593
3596
3598
3600
3606
3608
3614
3616
3618
3624
3627
3629
3631
3634
3637
3640
3644
3647
3650
3652
3654
3656
3659
3662
3665
3667
3672
3677
3680
3684
3686
3688
3692
3694
3696
3698
3703
3709
3713
3714
3718
3722
3724
3727
3729
3731
3734
3737
3741
3743
3747
3749
3753
3758
3763
3766
3768
3774
3777
3780
3784
3787
3790
3792
3798
3801
3804
3809
3810
3813
3817
3820
3822
3824
3827
3830
3835
3837
3841
3843
3845
3847
3848
3851
3853
3857
3859
3861
3869
3872
3874
3876
3877
3880
3884
3886
3888
3891
3894
3898
3899
3903
3907
3911
3916
3920
3925
3928
3932
3934
3937
3941
3946
3949
3952
3953
3957
3959
3961
3963
3965
3969
3972
3974
3977
3979
3984
3988
3993
3995
3998
4001
4004
4007
4011
4013
4017
4020
4023
4026
4032
4033
4036
4039
4042
4044
4048
4051
4053
4055
4058
4063
4068
4072
4077
4079
4083
4086
4088
4090
4093
4095
4096
4100
4102
4104
4108
4112
4114
4116
4118
4121
4125
4126
4129
4132
4136
4139
4145
4149
4150
4152
4157
4159
4161
4164
4168
4171
4173
4177
4179
4183
4184
4189
4192
4195
4202
4204
4209
4212
4216
4220
4222
4225
4227
4229
4232
4237
4240
4242
4244
4247
4249
4252
4255
4257
4261
4266
4267
4271
4274
4276
4277
4280
4281
4285
4289
4291
4294
4296
4300
4308
4311
4315
4318
4320
4322
4328
4332
4334
4336
4338
4340
4343
4346
4348
4349
4352
4354
4357
4361
4364
4368
4371
4374
4377
4380
4383
4383
4387
4390
4395
4397
4399
4404
4406
4408
4413
4415
4418
4422
4425
4431
4434
4438
4442
4443
4446
4448
4454
4457
4461
4463
4470
4474
4477
4480
4485
4489
4492
4496
4498
4500
4502
4504
4506
4508
4511
4515
4517
4523
4525
4527
4534
4539
4540
4542
4545
4551
4555
4556
4558
4561
4564
4568
4571
4575
4579
4584
4587
4588
4592
4595
4598
4601
4606
4611
4615
4618
4621
4624
4628
4633
4637
4642
4645
4648
4654
4657
4660
4665
4668
4670
4673
4680
4684
4686
4690
4690
4692
4694
4697
4701
4704
4709
4712
4715
4720
4721
4724
4727
4730
4732
4737
4741
4743
4744
4747
4749
4753
4755
4760
4763
4765
4768
4771
4777
4780
4782
4787
4789
4794
4797
4801
4804
4805
4810
4813
4817
4820
4821
4824
4826
4830
4832
4835
4838
4839
4843
4845
4847
4850
4853
4856
4857
4860
4865
4870
4872
4876
4879
4882
4885
4890
4893
4897
4899
4901
4904
4906
4908
4913
4917
4919
4923
4925
4927
4930
4932
4936
4938
4943
4944
4946
4947
4950
4953
4957
4961
4964
4967
4969
4973
4974
4976
4980
4983
4987
4989
4992
4994
4995
5000
5002
5004
5008
5013
5016
5018
5021
5023
5025
5030
5031
5035
5041
5044
5045
5049
5051
5052
5056
5060
5085
5093
5096
5100
5101
5104
5109
5111
5113
5116
5120
5122
5124
5129
5131
5136
5140
5146
5149
5151
5155
5159
5160
5163
5165
5167
5169
5171
5173
5174
5178
5180
5183
5186
5190
5196
5198
5199
5200
5203
5207
5209
5211
5216
5222
5227
5229
5234
5236
5238
5239
5241
5244
5249
5251
5254
5257
5259
5263
5268
5269
5270
5272
5277
5281
5282
5285
5288
5294
5298
5302
5303
5305
5307
5309
5312
5314
5316
5318
5325
5329
5335
5341
5343
5348
5349
5353
5355
5356
5359
5362
5365
5368
5372
5372
5374
5378
5382
5385
5389
5394
5398
5400
5404
5407
5408
5412
5413
5416
5419
5421
5424
5427
5429
5435
5439
5440
5442
5445
5448
5453
5456
5459
5462
5468
5470
5471
5474
5478
5484
5489
5493
5495
5498
5502
5507
5511
5513
5515
5519
5523
5528
5530
5536
5538
5542
5546
5550
5555
5560
5565
5568
5569
5571
5572
5574
5576
5579
5584
5590
5593
5598
5604
5609
5612
5617
5620
5625
5627
5629
5634
5637
5638
5641
5642
5645
5649
5652
5655
5659
5660
5663
5668
5669
5672
5677
5681
5685
5688
5691
5693
5696
5699
5702
5706
5710
5712
5716
5720
5721
5726
5730
5732
5735
5738
5740
5745
5748
5750
5756
5758
5759
5763
5768
5770
5771
5773
5777
5783
5785
5787
5790
5794
5797
5803
5807
5810
5814
5815
5818
5820
5824
5827
5829
5832
5836
5838
5840
5843
5848
5849
5854
5856
5861
5863
5865
5866
5869
5872
5875
5877
5879
5881
5890
5895
5900
5906
5908
5910
5912
5915
5917
5921
5923
5925
5929
5937
5940
5945
5948
5951
5956
5959
5964
5966
5971
5978
5980
5982
5985
5992
6000
6003
6006
6010
6014
6016
6018
6020
6023
6028
6032
6036
6040
6044
6048
6052
6054
6057
6062
6064
6067
6068
6069
6072
6074
6076
6078
6082
6084
6087
6089
6091
6096
6099
6100
6105
6108
6110
6113
6115
6117
6121
6124
6125
6128
6131
6133
6139
6142
6145
6149
6150
6153
6156
6158
6161
6163
6168
6171
6173
6175
6180
6184
6186
6187
6192
6195
6197
6201
6204
6207
6208
6214
6216
6220
6222
6224
6227
6229
6231
6235
6237
6238
6240
6246
6248
6249
6251
6255
6257
6262
6264
6269
6270
6273
6276
6279
6282
6285
6287
6290
6293
6296
6298
6299
6304
