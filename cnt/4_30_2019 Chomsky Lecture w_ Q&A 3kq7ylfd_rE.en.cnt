welcome everyone to the second lecture
this week by Noam Chomsky and I already
introduced him yesterday so you don't
need another introduction today let's
start off no welcome well - I'm
impressed I was told that everybody
would leave after the first lecture well
to recapitulate briefly of the many
different ways of in which one can
approach language we've focused on one
selected one particular one what's been
called the generative Enterprise which
is concerned with what's been called the
basic property of language namely that
each language constructs
in the mind and infinite array of
structured expressions each of which has
a somatic interpretation that cap
expresses a thought each of which can be
externalized in one or another sensory
motor system typically sound but not
necessarily as I mentioned this approach
revives a traditional conception goes
back to the 17th century went as far as
Earl early 20th century the last
representative was out oh yes person
then forgotten then revived without
knowledge of the tradition well within
within the generative Enterprise we also
have settled on a particular approach
which is not universal not required
namely what's called the bio linguistic
program that is regarding the language a
language that a person has as a trait of
the person property of the person sort
of like the person's immune system or
the person's visual system like other
such systems it is a particular instance
of something more general there is a
general mammalian visual system which is
innately determined there's a particular
visual system that's the result of
interaction with experience other
factors that govern growth and the same
is true of the internal language for
clarification called
I language to make it clear that there's
a particular technical notion involved I
fortuitously stands for internal
individual and also intentional with an
S namely we're interested for the
purposes of science in the actual method
by which this basic property is
expressed not just any method that
happens to yield the same external
consequences so that's the approach
within this approach there is a notion
at once of what constitutes a general a
genuine explanation a genuine
explanation will have to meet two
conditions the conditions of
learnability
the the system has to be able to be
acquired by an individual and also
evolve ability the innate internal
system the Faculty of language has to
have evolved so if device is used for a
description that can't meet those two
conditions that we have we don't have a
genuine explanation we have a
description descriptions can be very
valuable it's much better to have
organized descriptions than chaos but
short of genuine explanation now that's
a very austere requirement and until
recently it hasn't even been possible to
consider it but it is the goal to be
aimed at by any certainly any any
approach within the sciences or rational
inquiry generally well the early work
from the fifties distinguished to
different aspects of my language the
term came later but doesn't matter the
one of them was compositionality the
other was dislocation for
compositionality the approach the early
approach that was adopted was very
structured grammar heard dislocation
transformational rules it was assumed at
the time and in fact for a long time up
to the present and still is widely
believed that the dislocation is a
strange property of language you
wouldn't expect it it's kind of an
imperfection as it was called I have to
kind of explain it away compositionality
was taken for granted as I mentioned and
we'll talk about the day it seems like
the opposite is true which is kind of an
interesting result the phrase structure
grammar had two fundamental problems
which were already recognized by the 60s
one is it simply allows way too many
rules that alone disqualifies it can't
have a system that permits all sorts of
impossible rules the second problem
which was more subtle is that it
conflates three different aspects of
internal language which really should be
treated separately that's become clearer
over the years one is simply
compositionality structure having
structured the core elements of the
basic property
the second is linear order which is not
part of the basic property
the third is projection that is
determining what each structured element
is what kind of an element it is the
phrase structure grammar tries to handle
all three and it's become clearer over
the years that that's not the right way
to proceed
well the by the late 60s all of this was
becoming partially clear and this
approach turned to what was called x-bar
theory to overcome the problems of
phrase structure grammar
well x-bar theory takes care of some of
them it eliminates the problem of the
main problem of far too many rules in
fact there's basically no rules in an
x-bar theoretic approach so we don't
have all these impossible rules hanging
around it also distinguishes order from
structure so there's no order at an
x-bar theory there is still the
conflation of the projection and
structure which only recently in the
last couple of years was recognized to
be a basic problem primarily because of
the existence of the XS centric
instructions which are excluded by x-bar
theory although they're all over the
place as I mentioned in order to sort of
get around this annoyance special tricks
were invented to pick out the right
answer intuitively what is the right
answer but without any particular basis
so there's nothing an x-bar theory that
tells you that if you have an NPV p
structure you should regard it as
essentially verbal rather than
essentially nominal it's anything or is
an inflectional element totally
arbitrary and that of course is a very
serious problem it's overcome I think by
labeling theory which I'll simply assume
you're familiar with I don't have any
time to go into it but it's an approach
which
overcomes this problem doesn't and has
furthermore the advantage of the
accounting for when dislocation
operations movement operations and may
take place or must take place and when
they have to terminate does it
automatically
so that's but that all put aside the
transformation of the x-bar theory also
sort of fort although this was not
recognized at the time it forces you to
have a principles and parameters
approach because there's you have the
principle of x-bar theory but then you
have to somehow get the order that's in
particularly ages so English and
Japanese and sort of mirror images at
the core you have to sum house that
state that separately now there's an
interesting and that led right off
within a couple of years to general
principles and parameters approach which
raises interesting questions on its own
where did the parameters come from do
they have to evolve well if they have to
evolve then we're very far from genuine
explanations and in fact the simple case
that's x-bar theory exposes points to
what should be the answer so for the
case of say whether you have verb object
or object or a border that has it has to
be this state is for each particular
language but the answer to it has no
effect on the semantic interpretation so
it doesn't feed the conceptual
intentional level which is suggestive it
suggests that whatever the parameters
and if you think about the other
parameters that have been proposed say
null subject parameter or others they
they have the property that they are
they do not enter into determining
semantic interpretation so if you pick
say Ricci Keynes
proposals about a basic
subject-verb-object or versus a basic
subject-object-verb order doesn't make
any difference to the semantic
interpretation all of that is very
suggestive if the parameters don't enter
into semantic interpretation we have to
ask why that they are all together why
don't you just have the afterall the
core part of language is just expressing
thoughts basically having semantic
interpretations so it begins to look
right off as if the parameters should be
external to the core part of language
and I will come back to that but it
looks more and more plausible to me that
that's the case what about their
evolution well think again about the
head barometer it didn't evolve it's
just forced by the requirement of having
to externalize in a sensory motor system
that requires linear order so it doesn't
evolve at all it's just a requirement
there's a mismatch between the internal
system and the sensory motor system and
that mismatch just has to be answered
one way or another but there's no
evolution of the parameter and in fact
the goal ought to be to try to show that
all parameters of this property
there's very interesting work on this by
the eks group Epstein kitahara and
Seeley which studies studying particular
array of dialects of cup parroting
manager which they try to show that the
differences between the dialects simply
reduced to an unstated property of the
set of principles it allows alternative
orders of rules and turns out the two
several dialects just pick different
answers to this so for those if
that kind of approach can work for all
parameters then the problem of their
evolution has overcome and we go back to
having a hope of getting genuine
explanations well this is all these this
is all kind of work in progress in fact
not even in progress it's an open
problem to be investigated to try to
show that for for all parameters they
all of the options that distinguish
language they have the goal would be to
have two properties one to show that
they're simply part of externalization
they don't feed the conceptual
intentional level semantic
interpretation and secondly that they
don't evolve at all there are simply
options left open by core grammar which
have to be settled one way or another so
the language each language picks its way
of settling them that would be the
perfect answer as you can find it well
x-bar theory partially dealt with these
problems it still left the problem of
conflating projection and structure and
it had the huge problem of excluding
excess entry constructions which as I
say abound so that's no good
the the principles and parameters
approach that develop through the mainly
through the 80s led to a huge explosion
of the empirical work on a very wide
variety of type logically varied
languages I think more was learned about
language in the 80s and in the preceding
couple of thousand years also there were
conceptual steps forward but still there
was the problem of not being able to
reach genuine explanations well by the
early 90s
it appeared to number of people myself
included that enough had been learned so
that it might be possible to
tackle the whole general problem of
language on more principled grounds that
is to actually seek genuine explanations
to formulate what was called the strong
minimalist thesis which says that
language is basically perfect
can we still is called approaching
universal grammar from below
starting by assuming let's see how far
we can get by assuming a perfect answer
that would be a genuine explanation and
how much can we explain this way now
that's what's been called the minimalist
program now the way to proceed with it
is to start with the simplest
computational operation and just let's
see how far you can go where do you run
aground well the the what you have to do
is you have to explain the actual
phenomena of language on the basis of
the simplest computational operation
that would be the genetic element plus
other factors that of course enter into
acquisition of language and there are
several such factors one of course is
the data with which the child is
presented so we don't all speak Tagalog
it's gotta be something in the data that
picks out which language we end up with
and it should be the case that the that
the child the correct theory when we
converge on it will show that the that
very limited data suffices to fix the I
language there's an empirical reason for
that that is that the as psycho
linguistic evidence studies child
studies have increasingly shown the
basic properties of language are really
captured known quite early with very
limited
it's available there are other factors
sometimes called third factor properties
just general principles of growth and
development which are independent of
language maybe natural laws that would
for a computational system like language
the natural one to look at is
computational efficiency just assuming
that for the reasons I mentioned last
time that nature seeks the simplest
answer so computational efficiency plus
the simplest computational operation
plus whatever contribution data makes
that should yield the I like which
attained that's the goal this program
has been called the minimalist program
so where this can be achieved you do
have genuine explanation the if you can
if you if you can achieve if you can
explain something simply on the basis of
the first and the third factor namely
Universal grammar of the simplest
computational operation along with say
computational efficiency if anything can
be explained in just those two terms you
do have a genuine explanation you've
solved the learnability problem because
there's no learning okay that's the
optimal solution to the learnability
problem you you've also addressed the
evolvability problem in the best
possible way if you think it through the
basic property is simply a fact it's a
fact about the Faculty of language that
it satisfies the basic property if
therefore follows there has to be some
computational operation that yields the
basic property and whatever
computational operation it is it's going
to incorporate the simplest
computational operation and optimally
nothing else or not much else so if you
can reduce
something to the simplest computational
operation you have solved the problem of
evolvability even if you don't know the
exact answer as to how this rewiring of
the brain took place you know this must
have happened okay so you're free and
fair so whenever you can account for
something on the basis of just the first
and the third factor universal grammar
and third factor properties like
computational efficiency if you can do
that you certainly have a genuine
explanation you could have more complex
genuine explanations if you bring in the
effect of the whatever the right theory
of determining the island which from
data is when that factor of the second
factor is introduced you can have richer
kinds of genuine explanations but the
simplest kind will simply reduce to the
simplest computational operation plus
computational efficiency
well the notice of course that the
innate properties the simplest
computational operation these things may
be triggered by data by evidence but
that's normal for innate properties I
mean you don't grow you have to have
triggering evidence to get the system to
start functioning so in the case of the
visual system for example it's known for
the mammalian visual system including
humans that unless you have the right
kind of stimulation in very early
infancy in fact the first couple of
weeks pattern stimulation of certain
kinds then the visual system just
doesn't function it's not learning how
to function it knows how to function but
something's got a set off the system and
operation that's true for innate
processes altogether so presumably true
for language as well so for example if a
child was raised in isolation with
no noises the Faculty of language would
collapse presumably there's in fact even
some evidence for that for odd cases
unfortunate cases that have been
discovered but apart from that it seems
that we can we can we can at least begin
by asking how far can we get to genuine
explanation by just looking at first and
third factor well the so what's the
simplest computation possible well the
simplest computation which is found
somewhere buried in any computational
system that just takes two objects that
have already been constructed and
creates a third object from them in the
optimal case the operation will not
modify either of the elements that's
that is operated on it'll leave them
untouched and it won't add any
additional structure so it just takes
two objects column X and y forms a new
object Z which doesn't affect X doesn't
affect Y and doesn't add structure
that's basically binary set formation so
the simplest operation which has been
called merge in the last recent years is
simply binary set formation and the
question we can ask is if we can account
for something in terms of merge and
third factor properties like
computational efficiency we have in fact
reached genuine explanation for the
first time well if you think about the
operation merge binary set formation it
has two possible cases so we have x and
y we're forming the set X Y one case is
that x and y are distinct ok second case
is that one of them say X
is an element of why it's part of why a
technical term is a term of why we know
define the notion term to mean X is a
term of y if X is a member of Y or it's
a member of a term of Y normal inductive
definition okay reflexive so if one one
possibility is that X or Y doesn't
matter is a term of Y the other
possibility is that they're distinct
there there's a third imaginable case
namely that they overlap but if you
think about the process of construction
of binary sets you can't have that case
so that case we can roll out so we have
basically two options the names that are
used for them are external merged for
two distinct elements and internal merge
where one is part of the other the
linguistic analogues would be for
example forming read books from read and
books that's external merge internal
merge would be forming what did you see
from you saw what notice what you
actually form is what you saw what okay
you take what merger to you so what you
get what you saw what okay that's
critically important that's internal
merge internal merges dislocation but
this location is no a strange word
because you're actually keeping the
original element ok that's internal
merge external merge just takes two
separate things and brings them together
notice that these two are one operation
merge they're not two different
operations these are just the two
possible cases of the single operation
there's a lot of confusion about this in
the literature so you really want to be
careful about it well
just to be clear the minimalist program
is a program it's not a theory right
there's been a lot of you read in the
literature discussions of whether the
minimalist program is true or has it
been refuted or that's meaningless it's
a program of research that says asks how
far can we get towards genuine
explanation okay that's the program as I
mentioned last time by now it has some
in recent very recent years some
independent motivation from what's very
recently been discovered about the
conditions of evolution of the language
faculty to repeat quickly it's no been
established by genomic evidence that the
language faculty was established prior
roughly at the time of the art of the
appearance of anatomically modern humans
and there's no evidence for symbolic
activity before then so we have an
indication not a proof but an indication
that probably what happened is very
simple and it apparently hasn't changed
since since the language faculty is
shared among first of all among the
earliest separating groups but in fact
as far as we know among all human groups
so we have independent reason to believe
that it's that the program is a
plausible program it's likely that the
right answer to what the language
faculty is will in fact be something
like the simplest computational
operation well how far can we get on
this it seems to me that the last couple
of years there have been pretty
substantial achievements which give an
indication of how far we might be able
to go one achievement is cases where you
have reached general like some genuine
explanation I mean
a 1 is simply the unification of the two
fundamental properties of language that
were recognized back in the 50s
composition and dislocation if they can
both be unified in merge I think the
first person to point this out was he
said kita Hara who's sitting over there
if you can bring these together it's a
pretty substantial achievement these
were always thought to be two totally
separate parts of language but maybe
they're just the same operation in fact
the simplest operation so maybe the null
hypothesis the perfect case is that you
have both dislocation and composition
actually you can push that a little
farther if you think about the two cases
of merge internal and external which one
is the more primitive the simplest the
one that involves least computation
actually internal merge if you think of
external merge requires search you want
to bring two things together that are
distinct you have to search all the
available possibilities well what are
those first of all it's the entire
lexicon say maybe 50,000 items for
normal knowledge plus anything that's
already been constructed this is a
recursive procedure so you're
constructing things along the way and
any of them can be accessed for further
operations so external merge involves a
huge search procedure internal merge in
contrast involves almost no search at
all you're just looking at a particular
structure you're taking a look at its
terms that's it so internal merges by
far the more primitive operation
external merge is much more complex
which is an interesting conclusion
because the opposite has always been
assumed it's always been assumed that
compositionality is the obvious case we
are
explain that the dislocation is the
complicated case the imperfection the
strange property of language it turns
out when you think it through that the
opposites the case internal merge
dislocation is a simplest case which
have to ask is why do we have external
merge at all why not just keep to the
simplest case well you think about that
for a minute I'll come back to it in a
second but that's what you discover when
you begin to think about the nature of
the operations incidentally there are
some independent reasons for this not
proofs but suggestive if you think about
internal merge alone so imagine the
simplest case you have a lexicon of one
element and we have the operation
internal merge I could write this down
but it's simple enough so you can figure
it out in your heads so we have one
element let's just give it the name zero
okay we internally merge zero with
itself that gives us the set zero zero
which is just the set zero okay so we've
now constructed a new element the set
zero which we call one then we apply
internal merge again we take the term of
the set zero that's zero we merge it
with the set zero we now have a new
element consists of two elements zero
and the set containing zero we call that
one two we just keep going we've created
the successor function and in fact if
you think about a little more and we've
also created addition because if you
take one of these longer terms and you
merge that you're essentially getting
addition multiplication is just repeated
addition unbounded repeated addition so
we basically have the the foundations of
arithmetic just from internal merge
that raises a very interesting question
a question there is a question that very
much troubled Charles Darwin and the
co-founder of evolutionary theory Alfred
Russel Wallace who were in contact they
were both concerned with what appeared
to be a serious contradiction to the
theory of natural selection namely the
fact that all human beings understand
arithmetic if you think about it
arithmetic has never been used in human
evolution except in a very tiny period
tacked on at the very end but for most
for almost the entire history of human
beings nobody bothered with arithmetic
that you might have small numbers you
might have what's called numerosity
knowing that 80 things is more than 60
things but that's quite different from
arithmetic knowing that the numbers can
go on forever knowing what the
computations are of say addition and
multiplication so where did this come
from Darwin and Wallace simply assumed
that all humans have it by now there's
pretty good evidence that they're right
that it's even if you find indigenous
tribes that don't use have no number
words beyond maybe 1 2 and 3 and do
handle numerosity it turns out that as
soon as they were inserted into say
market societies they immediately
compute really they have the entire
knowledge and in fact on conceptual
groans we know that this must be the
case because there's no way to learn it
there's no possible way to learn that
the numbers go on forever
you can't have evidence for that but
every child and just knows it you know
so the problem is where'd it come from
since it was not selected and it's
universal looks like it contradicts the
fundamental ideas of the theory
evolution well a possible answer to this
is that it's just an offshoot of the
language faculty the language faculty
did emerge it's here we're using it so
it's around the language faculty at the
very least in its simplest case gives
you arithmetic so it's quite possible
that the reason we everyone knows
arithmetic is just we've got the
language faculty so therefore we have
the simplest case of it now on the
surface it looks as if there's counter
evidence which has been brought up quite
often namely dissociations kind of
things that's Susan Curtis discusses
there are dissociations between
arithmetic 'el competence and linguistic
competence so there are people who have
a perfect language faculty in count of
arithmetic and conversely however it's
not clear that that's a counter argument
for reasons that were pointed out a
couple of years ago by Luigi ritzy a
great language to most of us know he
pointed out that the tests for
dissociation or testing performance
their testing what people use not the
internal knowledge so as for example
there are also dissociations between
reading and language confidence there
are people who can have normal language
competence but can't read
conversely you know it can't be that
there's a separate reading faculty
that's impossible so in fact what it
shows is just that the tests are
studying the utilization of the
competence they're not testing the
competence itself so it may very well
turn out that this is the answer to the
Darwin Wallace problem a deep problem
the illusionary theory going beyond
there's other very suggestive evidence
Marvin Minsky one of the founder
of artificial intelligence for a good
mathematician about a couple couple
decades ago actually in the sixties he
and one of his students experimented
with the simplest Turing machines the
ones that the Turing machine is just you
know an abstract computer basically the
ones that have the smallest number of
states and the smallest number of
symbols they just asked the question
what happens if you just let these
things run free and they got an
interesting result most of them crash
they either get into infinite loops or
they just terminated over the ones that
didn't crash
they gave the successor function which
suggests strongly and then Marvin went
on to draw the plausible conclusion he
said probably nature in the course of
evolution will pick the simplest thing
so it'll pick the successor function so
he was thinking of things like
extraterrestrial intelligence he said we
can expect that if you ever find
anything like that will be based on the
successor function because that's the
one that it's the simplest one well
that's internal merge so from quite a
number of points of view we have a
conclusion that dislocation is probably
the primitive element of language
composition is the one that has to be
explained so why do we have external
merge all together well if you think
about suppose we didn't have external
merge at what would language look like
actually it'd be nothing but the
successor function because that's all
that internal merge can produce a
successor function press addition so if
you had say say ten elements in the
lexicon instead of one and used only
internal merge what you would get is ten
instances of the successor function
nothing else but language has other
properties for example it has argument
structure theta theory it has things
like agent
patient and so on well you can't get
that from internal merge you're required
to have external merge to develop the
structures which will yield argument
structure theta structure so an
empirical property of language namely
that it has argument structure forces
you to use this more complex operation
of external merge another reason for it
is again the existence of EXO centric
instructions like say subject predicate
you can't get those by internal merge of
course so there's just empirical reasons
that force language to include the more
complex operation of external merge
notice that this turns everything that's
been believed in the generative
enterprise and the tradition turns it on
its head that's exactly the opposite of
what's been assumed the basic operation
is dislocation internal merge arithmetic
essentially then because of special
properties of language like argument
structure EXO centric instructions you
just compelled to use also the more
complex case of external merge but again
notice that these are both the same
operation just two cases of the same
operation one more primitive than the
other you're forced to use also the less
primitive one compositionality because
of properties of language well notice
again that in the case of internal merge
you not only get this location but you
have several copies of the element that
has been dislocated if you those of you
who know how this works know that you
can have a great many copies you can
have successive cyclic dislocation which
leaves a copy in every position with
consequences
for externalization and interpretation
well that's very important because of a
phenomenon known and the linguistic
literature is reconstruction namely the
dislocated element is actually
interpreted in the position from which
it came and even intermediate positions
so standard examples are things like the
sentences it is his mother that every
boy admires so meaning every boy admires
his mother it is mother that admires
every boy doesn't work every does not
range over the variable his and the
reason if you think about it is that the
copy' what's reaching the mind actually
has the phrase his mother and the
position in which quantification works
every boy admires his mother is actually
reaching the mind although not the ear
there are many more complex
interpretations examples of this there's
no time so I won't go through it but if
those of you familiar with the
literature no you get quite complex
intricate examples of the phenomenon of
reconstruction now in a lot of the
literature the modern literature there
is an operation of reconstruction that
somehow takes the kind of structurally
highest element and reinterprets it back
in the initial position but that's
unnecessary because that comes
automatically it's just there okay so
what reaches the mind is the entire the
the what's called the chain of elements
all of the elements that have been
dislocated but it doesn't reach the ear
what reaches the ear is just cuts out
all but one of them actually that's
interestingly
it's not entirely correct we'll come
back to it in a minute but put that
aside for him and I'll return to it in
one second and notice that when you when
you look at the these cases you have a
strengthening of the conclusion that was
already suggested by x-bar theory namely
that the core language of the system
that just reaches the mind that gives
semantic interpretations that pays no
attention to the way the thing appears
on the surface okay it just asks what
happens on the internal operations
things may appear on the surface like
linear order or missing the copy but the
mind doesn't care because it's only
paying attention to the internal
operation and that raises the question
why does linear order even exist what
why do you have it well trivial answer
to that the sensorimotor system requires
it partially if you use a different
sensory motor system for externalization
like sign language it doesn't have
strictly linear order the reason is
visual space provides options that
acoustic production does not have you
have to speak one thing after another
but if you have visual space you can use
positions in it to convey the meaning
and that's in fact done in sign language
so an African sign language works by
picking us point in space and referring
back to it you can't do in spoken
language you can also have simultaneous
operations inside language like raising
your eyebrows while you're signing which
turns something into a question can't do
that in spoken language so the nature of
the engine general the nature of the
sense
remoter system is determining properties
of language in the broad sense which are
not part of core language at all they
just reflect the characteristics of the
sensorimotor system well now there's
something that came up in the discussion
last time I'll repeat it the
sensorimotor systems have nothing at all
to do with language ok these were in
place long before in fact the millions
of years before the scent of the
language ever evolved so they've the
time when language evolved maybe roughly
a couple hundred thousand years ago the
sensory motor systems were in place
independent of language they haven't
changed with the usage of language
they're still what they were you know
the origins of Homo sapiens with the
most extremely minor variations there's
no indication of any adaptation of
sensory motor systems to language so the
early homo sapiens who were who
developed this internal system by
whatever means they did some rewiring of
the brain which gave the simplest
computational operation they had the
task of getting this stuff out somehow
and they had to use the sensory motor
systems that are around as we do as
every infant does and the sensory motor
system imposes conditions but those are
not linguistic conditions they're not
strictly speaking part of language from
which we can conclude it's a contentious
claim as Hilda pointed out last time but
it looks from this kind of argument as
though things like linear order or
elimination of the copy that just don't
have anything to do with language
they're part of their conditions imposed
by the mode of externalization which is
language independent and as I mentioned
last time this has the paradoxical
conclusion that
just about everything that's been
studied in linguistics for the last
2,500 years is not language it's the
study of some amalgam of language and
sensory motor systems but if we want to
really think it through the internal
language
the thing that's computing away and
giving you somatic interpretations that
doesn't have any of these things it
doesn't have operations of
reconstruction doesn't have linear order
just operating along giving you
producing thoughts in your mind which
you can then try to externalize somehow
well in fact if you think about it
that's even true of what's called inner
speech so almost all the time say 24
hours a day you're basically talking to
yourself you can't stop it takes a
tremendous active will to prevent keep
you from talking to yourself but if you
think about inner speech its
externalized speech you're not for
example it has linear order or you can
think about two sentences you just
produced in your mind and you can ask do
they rhyme are they the same length
do they have copies and so on answer is
they don't so what we call in our speech
is very superficial it's not what's
going on on the mind all of this has a
lot of implications for the study of
consciousness and pre conscious mental
operations almost everything that's
involved in language seems to be
inaccessible to consciousness it's prior
to whatever reaches consciousness even
in our speech a lot of implications to
this which are worth thinking through
well a conclusion that kind of begins to
appear on the horizon is that core
language the internal computations
yielding linguistically articulated
thoughts a core language could
Universal it's possible that it's just
common to the species it's part of the
Faculty of language you know which would
solve the learnability problem because
nothing would be learned if that's true
and it would solve the evolvability
problem insofar as you can reduce the
operations to the simplest computations
okay so that's a kind of a goal that you
can see formulating itself on the
horizon something to strive at in
research you would also anticipate if
this is correct that the variety of
languages is just an apparent it's just
a surface phenomenon it's it's not
really a fact about language I mentioned
less time that in the in the
structuralist behavioralist period it
was assumed almost as a dogma that
languages can differ from one another
pretty much arbitrarily and that each
one has to be studied without any bias
any preconceptions that was what was
called the boolean doctrine if this is
correct that's just completely false
there's only maybe even just one
language and a lot of variation that
comes from mapping it into
externalization systems you would expect
the same to be true of the complexity of
language the relating to independent
systems is a complex affair so you could
naturally expect the complexity of
language to reside there also just the
fact that languages change very fast in
fact every generation there's language
change but could be that this is just
externalization anyway those are goals
to be thinking about kind of coming into
view when you just think about the
nature of the problem well under
externalization as mentioned before the
mind hears
every case but the ear only here's one
case okay so when you say what did John
eat for let's say it is his mother that
every student admires you don't hear it
is his mother that every student admires
his mother that's hitting the years so
you know how to interpret it but it's at
the month of mine but it's not hitting
the year and that's in general true
actually there's some very interesting
qualifications to this there's an
interesting study by it's known that in
some languages the initial position that
you don't hear in this location actually
has some kind of a phonetic mark some
sort of a sound that indicates I was
here you know there's a study by
Vietnamese language to a trend who goes
into this in some detail and relates it
to pers otic properties of the language
which pretty well determined when this
is the case more intricate for those of
you who know this stuff and success of
cyclic movement when you have successive
dislocation to higher and higher
positions there are in many languages
residues strange residues along the way
that's true at both of what are called
phases CP level and the VC P star P
level you get in some languages traces
that something went through here so
there's some indication in some
languages and the externalization that
there was something there but
overwhelmingly you just don't hear
anything you hear one case typically the
structurally highest case so there are
although what are called in situ
languages like say Chinese Japanese it's
you hear it in the base generated place
the base of the place where it started
from actually that's true in languages
like English too
so you have in situ constructions and
English and similar languages
in very special circumstances which are
interesting but overwhelmingly languages
very this way so the question that
arises is why don't you pronounce
everything
why bother deleting a lot of them well
here we turn to a third factor property
computational efficiency I suppose you
were to pronounce all of them notice
that the things that you could print I
gave a simple example what did you eat
but instead of what it could be of
arbitrary complexity so you know which
book that appeared then 69'd and then
was destroyed
did you read let's say if you repeat
that you've got a lot of computation
going on a mental computation to set the
stage for phonetic articulation and then
motor control to articulate the whole
thing so you have massive reduction in
complexity if you don't want it just
don't pronounce these things
okay so computational efficiency compels
of the deletion of the copies you can't
delete them all of course or else
there's no indication at the operation
ever took place so you have to leave one
at least typically they structurally
highest one then the rest you delete
well that's fine from the point of view
of meeting the condition of
computational efficiency but it has a
consequence for language use causes
problems for perception serious problems
in fact those of you who've worked on
mechanical parsing programs know that
one of the toughest problems is what's
called filler gap problems you hear our
sentence which begins with which book
then comes the whole sentence and you
have to find the place where which book
is interpreted but there's nothing there
that's called a filler gap problem these
can be pretty tricky I mean it's simple
in the case that I mentioned
but if you think through more complex
cases and become quite intricate so the
fact that what we have the following
strange situation a computational
efficiency is forcing the communicative
inefficiency okay computational
efficiency is forcing parsing problems
perception problems communication
problems what does that suggest it
suggests that the way language evolved
and is designed it just doesn't care
about efficiency of use it's of no
interest it just cares about efficiency
of computation now there's a lot of
evidence for this many things that are
quite familiar to us and nobody bother
thinking about but take things like
structural ambiguity you know examples
like flying planes can be dangerous
which could mean either the act of
flying planes can be dangerous or planes
that fly can be dangerous there's lots
of structural ambiguity in language
where does it come from well if you look
at the cases it comes from just allowing
the rules to run freely not caring
whether it causes perception problems or
not but all structural ambiguity
problems are perception problems you
have to figure them out take what are
called garden path sentences discovered
by Tom Beveren sentences like the horse
raced past the barn fell when you
present this to people they are confused
what's fel doing there you know the
horse raced past the barn was a sentence
it's called a garden path sentence it
leads you down the garden path but if
you think about it it's a perfectly
grammatical sentence it could be the
horse that somebody raced past the barn
fill ok well these these are all over
the place in fact incidentally they all
if you run the Google parser on them you
know the ones you can pick off the
Internet
all the time on these but but they're
normal parts of language and again these
are just cases where the rules run free
and they don't care whether it causes
problems or not actually the most
interesting case too complicated to go
into but many of you familiar with it is
Islands things where you just can't
extract you can think them but you can't
produce them the cases of islands that
are understood there's a lot of problems
with them all turned out to be cases
where you have computational efficiency
forcing the island compelling a
paraphrase of often a complex paraphrase
to somehow get around it well the
general conclusion seems to be that as
far as when as far as I know whenever
there's a conflict between computational
efficiency and communicative efficiency
computational efficiency wins the
language just doesn't care about
communicative efficiency that also has a
consequence there is a very widely held
doctrine almost a Dogma that somehow the
function of language you know how it
evolved its basic design and so on is as
a system of communication and apparently
that's not the case apparently language
just doesn't care about communication of
course it's used for communications for
a useful but it that just seems to be
something on the side it the probably
the background for this doctrine is
first of all that we do huge language
for communication all the time so on the
surface that's what you see and also a
kind of a belief going back to early
Darwinian notions that anything that
evolved had to go by small steps from
earlier things so somehow language
should have evolved from animal
communication systems but it seems to be
just
entirely false the language seems to
have come from something totally
different having nothing to do with
communicative systems well a lot to say
about this but time is running out I
think a my I've lost the track of time
what am i okay well there are if you let
me just go on a little bit because
there's a lot I want to get to there are
other genuine explanations some of them
pretty striking for non-trivial
properties of the most interesting one
of all I think is what's called
structure dependence
it's a strange property of language
puzzling property notice years ago that
the operations of language simply don't
seem to pay attention to linear order
they only pay attention to structural
relations so standard example and the
literature is things like the man who is
tall as happy if you turn that into a
question it becomes is the man who is
tall happy not is the man who tall is
happy okay that's obvious everybody
knows that well as soon as generative
grammar began the question arose why why
don't you use the simple computational
operation of picking the first
occurrence of the auxiliary and putting
in the front it's a much simpler
operation than the one that's used the
one that's used in fact picks the
structurally closest one so if you draw
the say the tree the structure of this
you see that's structurally the one
that's picked is the one that's used
similar take a sentence can animals that
fly swim we understand that to mean can
they swim not fly so it doesn't mean
I have to read it because it's hard to
work out is it the case that animals
that can fly also swim why doesn't it
mean that it's perfectly fine meaning so
why doesn't the sentence mean that
that's what it would mean if you picked
the first occurrence of ken and moved it
or interpreted it but it's not a mistake
that anybody can possibly make any child
makes let's say in fact it's by now
known that by about 30 months old
infants already are operating with
structure dependence it's something
that's just automatic you have
essentially no evidence for it and this
is true for all constructions in all
languages as far as anybody knows well
all these are things we take for granted
but the usual question arises why why
it's particularly puzzling because for
two reasons
for one thing we avoid the what looks
like the computationally simple
operation linear order that's trivial
pick the first thing is trivial we avoid
that we use a much more what looks like
a much more complex operation a second
reason it's puzzling is we ignore
everything we hear what we hear is
linear order we don't hear structure
that's something the mine constructs
internally so we ignore everything we
hear we ignore what looks like the
simplest operation and we do it
universally and this is true for cases
examples like is the man who is tall
happy have been a little misleading they
have led cognitive scientists to believe
that maybe you could learn this because
you could have something you have data
for it and this huge literature trying
to show that one way or another you
might acquire this but there's plenty of
cases with the evidence that just zero
total of zero so take for example the
sentence the guy who fixed the car
carefully practiced
it's ambiguous he can fix the car
carefully or carefully pack his tools if
carefully appears in front carefully the
guy who fixed the car or practice tools
it's unambiguous you unambiguously pick
the more remote verb not the closest
verb evidence for that is zero there's
no way you can learn that by deep
learning of the Wall Street Journal
corpus or something even that really but
this and this is universal and language
so there is a puzzle and there's in fact
an answer the answer is languages based
on the simplest computational operation
namely merge which has no order ok so
the option of using linear order just
isn't there in the use of and the
internal core language so here's a
situation where we have is there's no
learning necessary because that's just
innate there's no evolvability problem
because it's the simplest computational
operation so you have a genuine
explanation of quite a deep property of
language strange property of language
structure dependence no linear order
there happens to be confirming evidence
from other sources in this case there
are neuro linguistic studies the
initiated by a linguist - the linguist
here no Andrei immoral group in Milan
who brain scientists who devised the
obvious excellent of the study they took
two groups of subjects each of which was
presented with a invented nonsense
language one of the languages was
modeled on a real language that they
didn't know ii used rules that violate
universal grammar like linear order so
for example in the second language the
invented one the
Gatien would be say the third word in a
sentence okay which is very trivial to
compute and the one modeled on a real
language negation was whatever negation
is which is pretty complicated when you
look at it well what they found is that
the group the subjects who were
presented with the invented language
modeled on a real language handled it
with activation in the normal language
areas of the brain mostly Broca's area
in the case of the subjects presented
with the language say in which negation
was the third element they didn't get
Broca's area activation they got diffuse
activation over the brain which
indicates this just being treated as a
puzzle you know not a language problem
similar evidence was presented by a us
have studied people again the linguist
known Neil Smith and he on theat simply
who work with a particular subject who's
has very low cognitive capacities
extremely low but has remarkable
language capacities picks up languages
like a sponge basically and that when
they presented him Chris they call him
with the invented language based on a
real language learned it very quickly
when I presented him with the one that
has say negation in the third position
he it was couldn't handle at all it's a
puzzle he can't deal with puzzles so
here's a very striking case a rare case
where we have the optimal explanation
for something perfect explanation
genuine explanation fundamental property
of language we have confirming evidence
from all the other relevant sources
language acquisition snown right away
neurolinguistics you get the right kind
of brain activation psycholinguistics
get associations you couldn't ask for a
better result now here's a very curious
fact about the field of
cognitive science altogether there's a
huge literature in fact one of the major
topics in computational cognitive
science is trying to show how this could
be learned somehow from data well how it
could be learned without what's called
inductive bias for structural for
structure now you think of it that if
you look at this literature first of all
the methods that are proposed if they're
all clear clear enough to investigate
they all of course fail but a more
important fact is it wouldn't matter if
they succeeded so suppose that you could
could say take the Wall Street Journal
corpus and use some deep learning method
and figure out from it that speakers of
English use the structure structural
rule the significance of that would be
absolutely zero because it's asking the
wrong question the question is why is
why is this the case in fact why is it
the case for every construction at every
language so even if in conceivably you
could show how it could be learned
wouldn't matter okay it's a totally
irrelevant it's the wrong question what
about the inductive bias
there's no inductive bias it's just the
simplest explanation so it's as if this
entire effort which does a you know the
literature will know that this is a huge
part of computational cognitive science
but the entire effort is an effort to
try to disprove the null hypothesis now
if you think about it things like that
just don't happen in the sciences it's
just not the kind of thing you undertake
like if there's a perfect solution for
something you don't try to find some
complicated way in which you could have
gotten it some other way
it doesn't make any sense raises serious
questions about the nature of the field
it's a strange departure from science
there are other successes but starting
next time I want to turn to something
else
problems with merge the reasons places
where it really doesn't work
and what we can do about this
[Applause]
so when you talk about the when we have
I guess this internal computation biases
over the this computation efficiency
ways over the efficiency of use I was
wondering if the same saying apply to
for example the field of semantics where
when we formulate a sentence where we're
trying to parse a sentence based on its
meaning do we have the same this
internal system that weighs over the
meaning the using efficiency
she was asking since you mentioned that
language seems to favor computational
efficiency over efficiency of use or
communication do you think the same
thing applies with respect to semantics
when we're trying to interpret a
sentence and determine its meaning do
the same kind of principles apply that's
a very interesting question those of you
who are familiar with the field know
that these minimalist questions these
kinds of questions I'm talking about
have been focused on syntax okay there
are similar kinds of questions with
regard to phonology and there's a lot of
work in phonology in fact going back to
that
in general grammar back to the forties
trying to determine what are the
simplest approaches but nothing like
this has been done with semantics so
there's a lot of very interesting
exciting work on formal semantics one of
those lively areas of the field that we
just led the lots of extremely
interesting results but the questions of
conceptual the conceptual questions have
never been raised what is like those of
you know about this stuff say they're
full of lambdas and so on and so forth
but what is what what is all this stuff
about now formal semantics in fact is
syntax it's not semantics it's symbolic
manipulation it would become semantics
when you're related to the external
world that's no small problem if there's
time we'll talk about it later but the
internal symbolic manipulations are a
form of syntax which give you the basis
for potential semantics kind of the way
phonology gives you the basis for
potential phonetics you know it sort of
lays the basis from which relation to
the outside world could take place but
in this whole domain of the formal
semantics which again is quite rich very
exciting work these questions the
question of question you raise just
hasn't been asked what's the simplest
way of doing it and doesn't matter
the work is entirely devoted to try to
get an answer which is sort of the way
it ought to be the way we understand
things and here's the way we understand
the word only let's say so we make a
complicated description say and that's
the way it seems to work and connection
with negation or something but why this
way and not some totally different way
so that's a fair question is well if we
tried to ask the question
hasn't been asked for pretty good reason
that's hard question but if we ask the
question what is the computationally
simplest way to account for all of this
work in formal semantics and why does it
work this way and not some other way if
those questions are ever raised then
we'll be able to ask your question is it
the case here too that computational
efficiency overcomes efficiency of
interpretation similar questions arise
for real semantics that is relating
words to say what they refer to if
there's time at the end I'll come back
to this but it turns out that just to
give a foretaste that when you begin to
look at it the words of language do not
have the property of reference they
don't refer to things the way animal
symbols do in a lot of interesting
things to say about this we can ask why
but the kinds of questions you raise are
very pertinent but they're just not
formula yet until the task of developing
the whole domain of somatic
interpretation is subjected to the same
kinds of considerations and notice even
in the case of syntax it's very recently
only that it's been possible to pose
these questions in a way in which
answers are possibly forthcoming the
questions were posed very early but they
were way out of sight so it's actually
only in the last few years that you
really have examples you can point to
this in particular this question of the
conflict between computational and
communicative efficiency they couldn't
really have been asked not many years
ago because not enough was understood
about computational efficiency so those
are tasks for people like you that's the
future
Thomas mutter so if the language faculty
doesn't inherently care about being used
for communication purposes how do we get
from the internal merge or successor
function state to the point where we
motivate the usage of external merge how
do you see that link if I hope I hope I
can remember if I actually have lost the
first part if if internal merge is
primary over external merge right if
internal urge is primary and language
doesn't care about like the linguist
faculty isn't at its core about
communication and if the language
faculty isn't fundamentally about
communication then then how do we get
the point where external merge why do we
have external more well what the
language faculty if we accept going back
to the galilean challenge which I think
did formulate the problem correctly the
language is a means of expressing of
capturing and expressing thought now
thought has certain properties
independent of communication one of the
properties of thought is it does involve
notions like agent and action action and
the recipient of action action and goal
of action that involves all of those
notions that's part of our system of
thought we can ask where it comes from
now we're often evolutionary mysteries
but it's plainly there it may have there
may be things similar to that in animal
systems kind of hard to determine but
for humans it's plainly true so we have
what in the technical literature is
called theta structure argument
structure you know agent goal
and so on and so forth now in order to
capture these notions structurally you
need structures rich enough to express
the notions internal merge doesn't give
you that internal merge just gives you
the arithmetic it just doesn't have
these structures so you're forced to
have external merge simply to have the
basis for expressing thought to yourself
internally this is quite apart from
extra generalization forming thoughts in
your mind you're going to have to have
this kind of structure furthermore there
is the further fact of XO centric
instructions so you know like subject
predicate and just can't get this from
internal learning so properties of the I
think the right answer to your question
if we ever understand things enough will
be the properties of the thought system
require that you use the more complex
operation of compositionality otherwise
we would just be stuck with knowing
arithmetic question here and then
afterwards excuse me
so what do you think the role of
randomness is in within the field of
linguistics and do you believe such a
concept even exists such a is such a
concept of randomness not exists what do
you think is the role of randomness in
linguistics and do you think the concept
of randomness even exists or is even
applicable to language if you've random
this is not such a trivial notion but
let's say we understand it it obviously
exists in the universe does it have
anything to do with language well here
are some quite general questions arise
contemporary science this is also true
it's origin 17th century science pretty
much understood the concept of
terminus II and the concept of
randomness we'll suppose those are the
only factors in nature oh then we have
the conclusion that you never make
choices okay
you have no free will so you can't
decide to lift your finger
you couldn't have decided to ask that
question okay so a consequence of the
assumption that the world consists of
just determinacy and randomness is we're
automata
we're thermostats we just respond to
whatever the circumstances are now
there's a huge literature and philosophy
and the science is saying yeah that's
the way things are okay it's all based
on the assumption that all we understand
is determinacy and roundedness every one
of us knows that's not true you know if
we know anything at all we know that we
make choices okay other than that
there's nothing that we know that's more
elementary than that so we have this
strange situation something that we all
know to be true is inconsistent with or
at least doesn't fall within anything
that's understood within the sciences
well what do we do with that nobody
knows you know it's but there's no other
respect in which randomness enters into
certainly not the structure of language
but the use of language after I brought
up a case of this last time when I
pointed out that if you want to meet the
galilean challenge you have the problem
first of all of for finding the internal
language which generates an infinite
array of structures then kickings
if you want to speak picking one element
inside it and then mapping it onto
externalization the latter part is
within the framework of scientific
understanding the first part picking out
the element is not and that's true a
voluntary action generally it's
certainly not random we all know that is
it determined not in any way that anyone
can conceive oh so we're left with this
fundamental problem which was in fact
raised and discussed back in the 17th
century Descartes for example argued
that they said it's absurd to assume
that something that we know better than
anything else we know doesn't exist
mayn't merely because we have no
explanation for and of course that
raises rather fundamental questions
about human knowledge there's a kind of
a strange or something very common
assumption that humans are universal
creatures every other organism that we
know has limits on its cognitive
capacity so you take a say a rat for
example a rat can be trained to run all
kinds of mazes but it simply can't be
trained to run a prime number maze where
you turn left every prime number a note
no possible amount of training is going
to get to that because the rat just
didn't have the concept and that's true
for every organism so the question
arises is it true for humans or are we
somehow out of the organic world an
almost universal assumption which is a
pretty strange when I think is we're not
part of the organic world there is no
nothing that we can't solve in principle
but of course it's hard to know why
should we should believe them but I
think that's a very much the kind of
question that arises when you raise the
question you do goes in very fundamental
directions in the white trick
hello um so since linguistic linearity
arises from mapping merge which doesn't
care about linearity to the sound system
which necessitates linearity what
happens in the sense system also doesn't
care about linearity and I know we
talked about sign language but that
seems the not caring this about sign
language seems to be constrained because
you still have to make signs in a
sequential order so it still kind of
does scare about linearity in that way
and if we have this sense system that
doesn't care about linearity at all
would that produce the opposite
distribution of concatenative and non
concatenative morphologies or maybe just
only non concatenative ones I'll do my
best if you you you mentioned sign
language as an example of sensory motor
system that didn't require linearity to
the same extent that verbal language
does but the questioner points out that
it still has some degree of linearity
because signs some signs are you know
pronounced before others or you know
signed before others and if you had a
motive externalization that was
completely free of linearity
considerations would this favor was it
non concatenative morphology over
concatenative morphology morphology
speaking of morphology yeah uh-huh yeah
you mean morphology of putting markings
in a particular order weird or well
those are somewhat different but I mean
if you look at the morphology its
hartley externalization and if we really
understood morphology properly I think
we would discover that there are
elements that are not linearly ordered
they're kind of multi-dimensional you
know sticking at one or another way
that's not the way morphology is done
but of course morphology is based on the
traditional notion which also was the
way syntax was done that the fundamental
property is linear order now we have to
look at the linear order of things but
that's I think misleading for the
reasons I mentioned linear order just
doesn't seem part of language itself and
that probably extends to morphology so
if morphology was rethought abstracting
from the fact that things have to appear
in order we'd find something different
and in fact there are languages where
that's pretty clearly the case so take
the Semitic languages and just take the
basic route vowel pattern structure
that's not linear actually contemporary
phonologists do try to treat it as
linear so if you look at say John
McCarthy's work on optimality theoretic
approaches to Semitic he does treat it
as linear but I think that's pretty
misleading a highly inflected language
like say Hebrew which happens to be the
first language for which a modern
generative grammar was written back in
the 40s it simply does not have linear
order in the morphology take a look at
broken plurals in Arabic for example
there's nothing linear about them I mean
you sort of have to if you want to write
it on a blackboard it has to be linear
but it just doesn't make any sense these
are the whole structure the word has
changed in a fixed way if it's
quadrilateral say to get a broken
but the bells changing all sorts of
things so we kind of force it into the
frame of a blackboard or a piece of
paper but that's pretty much irrelevant
to the language if we really want to
study the language in a fundamental way
we've got to abstract ourselves from
that and I think the future morphology
if done properly you'll avoid it sorry I
think there's a follow-up Tim oh I'm
sorry I think I have a way of
disambiguating my question and it's
does the
necessity but that does the necessity of
producing sounds in an or in an order
have any effect on the distribution of
concatenative news morphology again
light as opposed to non-competitive like
the try continent flutes of Arabic and
broken plurals because the distribution
is that's less frequent
is that right does does the necessity of
pronouncing words in a particular order
cause I'm going to reorder what you said
a little bit but is that perhaps account
for the fact that was it non
concatenative apology is less frequent
yes
the non-college is less written well I
don't know if it's less frequent I mean
you know you take a look at the end of
Europe not the languages don't have very
high levels of inflection like say you
take a look at English for example which
was really the the main language that
was studied in distributional structural
linguistics American linguistic studies
say Harris's methods studied basically
English and in English it's sort of true
not entirely but largely true that the
morphemes appear kind of one after
another obviously not really true like
not true for irregular verbs but there's
plenty of other languages where that
just isn't the case you know a Semitic
or striking example where the and in
fact the most extreme example I know
where the you take a word structure and
there's just many factors integrating
the root bowel pattern the whatever
turns it into past tense all of this is
kind of interacting in a way which ends
up being something that's in linear
order but that's just because it's for
two so I'm not sure it's true at all we
should move on to another questionnaire
thank you you mentioned that in the
absence of input various faculties would
collapse so children who cannot see from
birth and then are given site later
frequently do not have the capacity to
do it or wild children who are to reduce
the language you mentioned that the
language faculty should collapse you
also mentioned that the dissociation
between arithmetic ability and
linguistic ability should be due to the
fact that it's the externalization
system and not in fact the innate
language faculty that is being dissolved
and so my question is
with wild children do you think that
it's the externalization capacity that's
never generated or in fact the entire
language capacity itself and because of
that would you expect wild children to
not be able to have an arithmetic
capacity as well if they are tied
together
if no-no-no if a is just complicated
so given I'll give you a loose summary
given the connection that you drew
between our medical ability and I think
it was specifically internal merge that
is the question is in the case of a wild
child that is to say a child not exposed
to language data until past the critical
period would you expect just the
externalization aspect of things
to be permanently impaired or would you
expect the internal language also to be
permanently impaired because of lack of
input well first of all should be clear
that there's basically no evidence
because there aren't wild children they
wouldn't survive you know the there are
clinical cases that come pretty close to
this the best known best studied one is
a girl called Jeannie maybe some of you
know about this is a kid who was
discovered at about age 12 or 13 I think
tied to a chair in a room where she was
had lived almost her entire life without
any external input path you know crazy
psychopathic parents had tied her to a
chair and stuck her up in an attic
somewhere they shoved food at her so she
didn't starve when she was found of
course she was immediately really
Susan Curtis who's here was her
therapists and psychologists and studied
her in detail and it ended up being a
pretty tragic case she ended up kind of
a vegetable but that's another story but
it seemed at first that there was Susan
did try hard to see if she could get her
did all the language capacity and it
seemed at first that there was some
success in this but the more she studied
it the more became clear that jeanne who
was very clever was using sophisticated
techniques to make it appear as if she
used an understood language but it
really wasn't working in fact it's very
much similar to the case of in this
respect to the case of the best cific
most sophisticated effort to train an
ape to speak the case called NIM where
the chimpanzee was raised from infancy
as close as he could come to the way a
child would be raised with intensive
training and again it looked at first as
if them animal was getting something
like language but it turned out that was
essentially nothing I was just randomly
introducing signs which the Institute
the trainer's thought meant something
that didn't mean anything to the animal
so but you don't know what conclusion to
draw from this because for several
reasons for one thing Jeannie was
plainly psychotic and nobody could go
through that experience and be anything
like a normal person so there's all
kinds of psychological problems onion
off those are what's are involved or the
language linguistic problems are
involved there's another problem
Jeannie had I think two years of
language exposure before she was
sequestered now that's a very striking
fact because
from the study of there are a number of
cases not a huge number of cases of
people who lost sight and hearing
completely usually spinal meningitis but
we're able to be trained to recover
something like linguistic capacity most
famous cases Helen Keller who became
pretty lucid in some question about just
how much but substantially every case
that's been the way they the training
works is by a system called Atta DOMA
which actually Helen Keller sort of
figured out for herself what the trainer
does is put the hand on the face of the
subject like this with the thumb you can
determine whether the vocal cords are
moving and with the fingers you can get
something about the shape of the face
and from that really elementary data the
people just do develop considerable
language capacity in fact in some cases
it was found that you have to go to
pretty complex structures like say tag
questions to find somebody didn't
understand these are people who hold
jobs who live by themselves and travel
by themselves you know really function
in society
it's an astonishing case of the
acquisition of rich data from a capacity
from extremely limited data however here
comes the qualification in every case
where there was success the disease
struck after about 18 months old now
there aren't enough cases to say publish
an article about it but it Helen Keller
is a case in point I think she lost
sight in hearing at about 20 months what
all of this suggests is that a kid of
about 18 months old Laurie knows the
language it can't exhibit anything
the people who studied child language
the better the techniques get for
studying child language the earlier it
turns out that kids know the things so
it's quite possible that say by age you
know two or so it's kind of all in there
not the vocabulary of course but the
basic features of the I language and
after that it just sort of comes out as
the memory grows and child matures and
so on I mean it's it has been well
demonstrated Lila kleitman or others
that kids of what's called the two-word
stage you know when they're just
producing things like two-word sentences
they actually understand seven or eight
word sentences if you mix up the words
then or they think don't understand that
and they know all the function words
even though they don't use them they
know where they belong so so basically
that's the state of the empirical data
if you know if there was any other
animal that had anything like the
language capacity anything remotely like
it you could do experiments to determine
what happens under controlled
environments but there's no other animal
there's nothing like the language
capacity and you just don't do it with
humans you can imagine experiments but
you wouldn't do them so I think the
answer is we can guess but we don't know
if you look at other faculties say the
visual faculty where you do should study
animals there it's pretty well known
that if you deprive a mammal say a cat
or a monkey of the patterned stimulation
you don't say crossed lines and things
if you deprive them of that in the first
few weeks of infancy the visual system
just deteriorates the cells of the
striate cortex just degenerate
and in fact it's quite interesting if
you put a say a ping-pong ball over the
eye of a kitten so it's getting diffused
light but no patterns the system
degenerates if you give it lines in a
particular direction the visual system
will mature in such a way that it
recognizes lines but not lines in the
horizontal direction this is classic
work by David Hubel and Curtin Wiesel
which they got the Nobel Prize but and
that seems to be true of every system we
understand it has to be kind of
triggered by particular kinds of data in
order to function and it's reasonable to
assume that maybe that's true of the
language faculty and maybe it's all
happening in the first year or two
question here in the middle
Thank You professor
the purses do you think that by dog
self-referential sentences and the
ability of humans to process them is
point within us or is the it is
something that we can learn and if we
are learning it then what is the what
could be that process or how can we
figure out that process the subject is
self referential paradoxical sentences
and if humans are born with it or do we
learn thank you a question about self
referential sentences paradoxical
sentences long till I like for example
sorry this sentence is a lie there's a
boy or a bird yeah do you think do you
think we're born with the ability to
understand these or does it have to be
learned well I don't think it can be
learned how would you teach somebody
I mean we do understand those paradoxes
the first time they're produced we see
that they're paradoxical of course they
lead into all sorts of incredible
results in mathematics like girdle's
theorem and so on but the simple case
you know all Cretans are liars and so on
this sentence is a lie we do understand
it right away
a pillock eight godel's incompleteness
theorem you obviously need to have a
formal system to begin with which we
could look at mathematics as a system
beginning with successive functions
wouldn't in humans you would require
that foundation from within
before you can we can talk about the
interpretation of paradoxical self
referential sentences
what so in order I'm sorry can you start
at the beginning again I just lost the
beginning in order to create a paradox
you would have to have a formal system
that you have to learn so or in order
let me do it as you do it so I don't so
in order to understand the liar paradox
for example you need to have a formal
system that you have learned a formal
mathematical system that indices
internal at least that is internal so so
you have to have a logical system of
some kind but we do have an innate
logical system so in fact experiments
with infants show that they understand
things like say de Morgan's laws and
actually some interesting work on this
if you want to follow Steve Krane very
good cognitive scientist he's in
Australia has studied the way in which
infants interpret the Morgan's laws in
English and Mandarin and it turns out
they do a little bit differently related
to the structure of the languages as he
shows but they do make complicated
inferences based on the internal
arrangements of the logical particles
the same with quantification and others
so all of this stuff is in there
somewhere and it means we have logical
capacities which are being instantiated
in language actually things like the
liar paradox had interesting
consequences for philosophy of language
like great linguists logicians likes a
very good or an alfred tarski rudolf
carnap
all concluded that it's just not worth
studying human language at all because
it contains paradoxes so let's just
throw it out and study formal languages
if you look at say
Quinn's work take his book word and
object very influential book the the
first couple of chapters of the book
deal give a kind of radical behaviorist
interpretation of language the rest of
the book is about what he calls
regimented language so let's throw away
all the nonsense of ordinary language
and make a formal language which has
none of these properties Rudolf Carnap
who I knew personally has told me once
he just doesn't understand why people
even speak language why don't they speak
a formal language which as none of these
crazy properties you know so but yes
it's it's true that language permits the
expression of all sorts of
contradictions which for a logician is a
nuisance why don't we speak the theory
of types or something like that actually
had a very funny experience just with an
old trend the Israeli logician and
yahushua bar-hillel some of you know
it's a very close friend for many years
but he came to the United States around
1950 about the same time I got the
Cambridge Harvard we met very quickly
became long-standing crumbs but he was
learning it he sort of know a little
English but he was learning English here
or me I remember he came to me once and
he said he'd made a great discovery he
that he was gone when it good when it
was going to go back to Israel he was
going to explain to the people in Israel
that you could that they could remove
from Hebrew all sorts of inflections
which aren't used in English but
everyone gets with gets along fine
without them so he was gonna go back and
explain let's stop wasting the next time
he came back to visit I asked him how it
worked
couldn't convince anybody
over here by the wall oh yeah I was just
wondering why do you think we've
developed this internal computational
efficiency if it wasn't for the sake of
unikitty of efficiency just why did we
develop that like just as a species
assuming as you say that language was
developed not for the purpose of
communication why did we bother to
develop it is that a fair summary yeah
well and remember evolution doesn't
happen for reasons it just happens you
know things happen you know you get a
mutation most of them are lethal so they
disappear some of them are okay of the
ones that work most of them disappear
anyway some survive that's why you get
changes in organisms that way that's why
we're not all bacteria because over the
millennia you know some the changes have
just stuck they kind of worked and
people reproduce and so on so the same
gotta be true of language is true of
everything else it's not mutations don't
happen for a reason okay they just
happen that's where randomness enters
there are random duplications they lead
to some changes in the genome either
they work or they don't work
something happened in early when early
modern humans developed what happened
what happened is a very interesting
question there's some ideas about it
there's a book by Angela fritter Ricci a
very good neuroscientist it's kind of a
state-of-the-art book about the state of
the art of the neurolinguistics I forget
the title of the book you may recall out
of it a year I don't you know two ago
but what she has done she's a German
cognitive neurologist she's done very
interesting work
I have to add I wrote the introduction
of the book I have a stake she's done
extremely interesting work which
basically argues that there are certain
major areas of the brain that are
involved in normal language use the
dorsal and ventral area and they have
circuits connecting them and which she
has found in her studies is that in
monkeys this circuit is not complete in
adult humans it is complete complete
means just myelinated fully myelinated
so it actually functions and she has
evidence that in infants the malla
nation is not complete so the circuit is
not complete but through early
maturation the circuit becomes complete
and she's kind of correlated the stages
of completion of the circuit with stages
of language acquisition and there are
some interesting correlations which
suggests that maybe what happen is that
some unknown mutation took place in
early modern humans which just led the
maturation of completion of this circuit
and maybe that nobody understands enough
about neuroscience to say what neural
structures yield this and that
consequence but could be that whatever
happened there led to something like
merge it's arguable you know but it's a
very hard copy and not much is known but
like that's a gives you an example of
the kind of thing that might be true and
that you might investigate even and even
with current technology that remember
current technology is quite primitive so
ever you know the imaging technique
gives you a lot of information but it's
very superficial information it's
information about things like blood flow
what's going on on the inside of a
neuron nobody has a clue you know I
meant assumed in much of literature that
neural nets are the things that do
computation but there's pretty good
reason to believe that that's not the
case they just don't have enough
computational capacity and in fact
what's inside the neuron things like
what are called microtubules and so
they're complicated things inside every
neuron or even things inside the
molecules of the genome like RNA and so
on they have huge computational capacity
that way beyond what neural nets have
and it may be that we're you know that's
the kind of place you have to look at
the kind of what the brain is doing a
lot of this is very much up in the air
and the technology well sophisticated is
nowhere near the point where it can
really answer these questions but there
are interesting proposals in the lower
which we're investigating but kind of
lost track of your question but that's
it I think those are the directions
where you kind of have to go to find
answers to these things
one last question over here then we
should so I'm from an engineering
background but I I was interested
actually by a few questions is that for
infants that are born deaf
there are some technologies now that
they the infants were breast so whenever
there is sound the best vibrates so it
gives a some sort of a sensory system
for the or an input or therefore their
infants do you think these technologies
will work or what suggest what
technology do you also suggest to put
this input system for sensory for babies
who are like born deaf or well there's a
lot of interesting but maybe you know
this stuff but
that's quite um I know many people here
at GU there's a lot of very interesting
work on infants born there and it turns
out that some of the main work on this
was done by a Laura potato very good
cognitive psychologist what she's found
is that deaf babies learn sign almost
exactly the way hearing babies learn
sound they go through the same stages
same ages some really remarkable results
so for example the babbling stage kid of
about six months old is babbling all
I've done the deaf kids are going like
this all done and it's not just random
gestures it's the kind when you study
the gestures it's the kind of gestures
that enter into sign language that when
you get to one of her most striking
results was with deaf kids of about I
think fifteen months old the parents
here will know that when a kid is about
that age they tend to interchange probe
pronouns first and second pronoun so
they use the first-person pronoun to
refer to the interlocutor you know the
mother you say I that's the mother you
is me you can understand why that's what
they hear yeah you as welcome to me I so
they interchange them turns out that
deaf kids do exactly the same thing
which is very striking because they have
an iconic gesture of pointing and they
have an iconic gesture the same one in
fact for the pronoun the one for
pointing they get straight the one for
the pronoun which is the same gesture
they invert so whatever
hearing children are doing visual
learners are doing exactly the same
thing and they end up in the same place
so they end up with the same language
now there's also work on blind kids
mainly Lila quite none in my barber
lando book on this and essentially the
same a blind child learns the visual
vocabulary at the same rate as seeing
children learn the visual vocabulary and
they do it with interesting differences
so if you take si and look at for a
blind child
si will cover the whole will basically
mean understand so a blind child can't
understand why her mother can't see the
back of her head cuz she can just go
like this look at just means point
through but si means sort of grasp what
it is and then it just goes on like that
by the time you need the graduate
students who have the most amazing
refined understanding of the most
detailed visual words you know glance
essentially it just all comes out
whatever the restriction on mobility is
so it just it just seems like it's all
inside there waiting to be stimulated in
some fashion and then it goes through a
startup well there was an effort I don't
know what the state of it is these days
but a psychologist name was Baker she
and he was actually at MIT about in the
50s or 60s decided to experiment to see
whether if he took a deaf child and you
construct
vibrating belt kind of which you put on
some part of the body which has pretty
sensitive nerve sensitivity could the
child pick up from that totally
different modality the same thing that
you're hearing from the basilar membrane
and there were some experiments with
this they the last ones I saw over a
long time ago maybe 40 or 50 years ago I
think you were getting some weak
evidence that maybe the kids were
distinguishing language from not
language but as far as I know nothing
went beyond that maybe some of you know
more about this but as far as I know
these efforts haven't at least I haven't
seen anything that pursued them since
then okay thank you all for coming thank
you
[Applause]
10
14
19
21
22
45
47
52
56
59
63
67
71
74
77
81
84
89
92
96
100
103
107
112
115
120
123
127
131
135
139
143
145
149
152
155
158
162
166
169
173
176
179
182
186
190
191
193
198
202
206
209
212
215
219
222
225
230
233
236
238
241
244
246
249
253
255
259
262
264
267
269
271
274
277
280
283
287
290
293
296
301
305
308
312
314
317
320
324
328
330
333
336
339
341
343
346
350
352
355
358
362
365
368
373
376
379
381
386
389
391
395
397
401
403
406
408
411
414
417
418
422
425
426
430
434
438
442
443
447
450
453
456
458
461
466
469
473
477
480
482
486
488
491
493
496
500
503
506
509
512
515
517
519
522
524
527
530
535
536
539
541
544
547
550
553
557
559
560
565
568
571
574
576
580
582
585
588
591
593
596
598
601
605
608
611
614
616
621
626
629
633
638
640
644
647
649
653
655
657
660
663
666
670
673
674
677
681
683
685
689
692
696
698
701
704
707
710
714
717
721
724
726
729
733
736
741
746
749
751
754
757
759
762
765
767
770
775
778
782
787
790
794
798
801
804
807
809
811
815
818
821
823
826
829
832
835
838
842
845
848
850
853
856
860
862
865
867
871
876
880
884
887
890
893
896
900
903
907
909
912
914
918
922
924
928
929
933
937
939
943
946
950
954
957
959
961
963
968
970
973
975
978
981
984
988
991
996
1002
1005
1008
1012
1014
1018
1022
1025
1029
1032
1034
1039
1043
1049
1053
1056
1059
1062
1065
1068
1072
1073
1077
1081
1084
1087
1090
1093
1096
1099
1101
1105
1108
1112
1115
1118
1121
1125
1129
1135
1138
1141
1147
1149
1152
1155
1158
1161
1163
1165
1168
1171
1174
1178
1181
1183
1187
1189
1192
1195
1198
1200
1203
1206
1207
1209
1212
1215
1218
1221
1224
1226
1230
1233
1237
1240
1241
1245
1249
1253
1256
1258
1261
1263
1269
1272
1274
1278
1281
1283
1286
1290
1294
1298
1301
1304
1306
1309
1312
1315
1318
1320
1323
1326
1329
1332
1335
1338
1341
1344
1347
1351
1353
1357
1359
1363
1366
1371
1375
1377
1381
1384
1387
1393
1396
1399
1402
1404
1407
1411
1414
1416
1420
1423
1426
1430
1434
1437
1440
1443
1446
1450
1453
1457
1462
1466
1469
1473
1477
1480
1485
1487
1492
1495
1497
1500
1503
1506
1509
1512
1515
1518
1523
1526
1530
1532
1537
1540
1544
1549
1553
1557
1561
1564
1568
1571
1574
1577
1580
1582
1586
1589
1591
1593
1596
1598
1602
1606
1609
1612
1615
1617
1621
1624
1627
1631
1635
1637
1641
1644
1648
1652
1655
1660
1663
1666
1668
1671
1674
1677
1680
1683
1685
1688
1691
1695
1697
1701
1703
1706
1709
1713
1716
1718
1722
1724
1732
1734
1735
1739
1742
1745
1749
1752
1755
1757
1761
1764
1766
1769
1772
1775
1778
1783
1786
1789
1793
1797
1799
1802
1807
1809
1811
1814
1816
1820
1823
1825
1827
1829
1832
1836
1839
1842
1845
1847
1851
1854
1856
1859
1862
1865
1868
1868
1872
1875
1878
1879
1882
1885
1888
1891
1894
1895
1899
1901
1903
1907
1912
1916
1920
1923
1926
1928
1930
1933
1940
1944
1949
1951
1957
1960
1964
1966
1969
1972
1976
1979
1981
1984
1987
1989
1992
1996
1999
2002
2005
2010
2015
2018
2021
2024
2027
2030
2033
2036
2039
2042
2046
2049
2051
2054
2057
2060
2062
2065
2068
2071
2073
2076
2079
2080
2085
2089
2093
2097
2098
2101
2105
2107
2110
2112
2115
2118
2121
2124
2127
2130
2134
2135
2139
2142
2145
2148
2151
2154
2158
2161
2163
2166
2168
2172
2174
2178
2180
2183
2187
2189
2192
2196
2198
2202
2204
2207
2210
2214
2218
2221
2224
2228
2230
2233
2236
2238
2241
2243
2246
2248
2251
2255
2259
2262
2266
2269
2272
2275
2278
2281
2285
2287
2290
2293
2295
2297
2299
2301
2304
2307
2309
2310
2313
2317
2320
2323
2326
2329
2331
2334
2335
2337
2339
2342
2346
2349
2352
2355
2357
2360
2363
2365
2367
2370
2372
2375
2378
2381
2384
2387
2389
2393
2396
2399
2402
2404
2407
2410
2413
2416
2419
2422
2426
2429
2432
2435
2438
2441
2445
2449
2453
2455
2458
2461
2465
2469
2472
2475
2481
2485
2489
2492
2495
2498
2501
2504
2508
2511
2515
2520
2525
2526
2528
2532
2534
2535
2538
2542
2544
2549
2551
2554
2557
2562
2566
2571
2574
2580
2583
2585
2588
2591
2595
2598
2601
2604
2606
2609
2610
2613
2617
2619
2622
2624
2629
2632
2634
2637
2640
2646
2649
2651
2653
2658
2662
2666
2669
2672
2674
2678
2681
2685
2690
2694
2697
2700
2703
2707
2709
2712
2715
2718
2723
2727
2732
2735
2737
2742
2744
2748
2750
2754
2758
2761
2763
2767
2769
2774
2778
2780
2784
2787
2789
2792
2796
2799
2799
2803
2807
2810
2813
2815
2817
2820
2823
2826
2830
2832
2835
2837
2840
2843
2845
2849
2851
2854
2857
2860
2864
2868
2870
2873
2876
2879
2882
2884
2887
2890
2894
2896
2899
2902
2906
2909
2913
2916
2917
2922
2925
2928
2931
2933
2935
2938
2942
2946
2949
2952
2953
2955
2959
2961
2963
2965
2968
2971
2973
2976
2979
2984
2987
2990
2993
2996
2998
3002
3004
3007
3010
3013
3016
3019
3023
3027
3029
3032
3036
3038
3041
3045
3048
3050
3054
3058
3061
3064
3067
3070
3074
3076
3078
3079
3083
3087
3089
3093
3096
3099
3103
3106
3109
3112
3116
3119
3121
3125
3128
3131
3134
3137
3140
3142
3145
3149
3151
3155
3157
3162
3166
3169
3172
3175
3177
3180
3183
3187
3190
3194
3199
3201
3204
3208
3210
3214
3217
3221
3223
3224
3227
3229
3232
3235
3240
3243
3246
3250
3252
3256
3259
3262
3264
3268
3271
3273
3275
3279
3282
3286
3291
3294
3296
3298
3300
3302
3304
3308
3311
3313
3316
3320
3324
3326
3327
3330
3334
3336
3339
3341
3343
3346
3349
3354
3357
3360
3362
3365
3367
3371
3374
3375
3378
3380
3384
3388
3391
3395
3397
3398
3403
3407
3409
3411
3414
3416
3421
3424
3426
3429
3432
3434
3438
3441
3444
3448
3451
3454
3456
3459
3463
3465
3467
3469
3472
3474
3476
3480
3484
3487
3490
3493
3497
3500
3503
3506
3508
3511
3513
3517
3520
3523
3527
3530
3533
3535
3537
3540
3543
3546
3549
3552
3555
3559
3564
3567
3570
3573
3575
3578
3581
3583
3585
3587
3591
3594
3597
3599
3600
3604
3606
3609
3613
3615
3618
3620
3623
3626
3629
3631
3634
3637
3639
3643
3646
3649
3652
3656
3660
3662
3664
3668
3671
3675
3678
3681
3685
3687
3690
3693
3696
3702
3704
3707
3710
3712
3718
3722
3725
3728
3730
3733
3734
3736
3738
3740
3743
3746
3749
3759
3761
3763
3767
3770
3773
3775
3777
3781
3784
3787
3789
3792
3796
3799
3803
3807
3811
3814
3818
3822
3825
3827
3830
3832
3835
3837
3840
3842
3844
3847
3851
3855
3860
3862
3865
3868
3871
3873
3876
3879
3882
3886
3888
3893
3896
3899
3900
3903
3907
3911
3913
3917
3920
3921
3924
3926
3929
3933
3935
3938
3939
3943
3945
3948
3951
3953
3956
3960
3963
3966
3970
3973
3974
3977
3979
3981
3983
3987
3992
3994
3998
4001
4005
4007
4011
4013
4016
4019
4022
4025
4027
4032
4036
4038
4043
4046
4050
4053
4055
4058
4061
4063
4065
4069
4072
4077
4080
4083
4086
4089
4092
4098
4101
4105
4108
4111
4114
4119
4122
4125
4127
4130
4133
4135
4138
4140
4143
4146
4149
4152
4155
4157
4161
4164
4166
4170
4173
4175
4178
4183
4184
4187
4190
4194
4197
4199
4202
4205
4209
4211
4214
4216
4220
4223
4226
4230
4232
4236
4239
4242
4246
4249
4252
4255
4257
4260
4262
4265
4268
4272
4278
4280
4283
4286
4289
4292
4295
4298
4300
4303
4307
4310
4314
4318
4320
4323
4326
4330
4332
4336
4338
4341
4344
4346
4349
4353
4356
4358
4361
4364
4368
4370
4373
4375
4377
4380
4382
4384
4387
4390
4393
4396
4398
4399
4402
4403
4408
4442
4449
4454
4458
4462
4465
4468
4473
4475
4478
4481
4489
4493
4496
4499
4502
4505
4508
4512
4516
4520
4522
4527
4529
4535
4538
4540
4542
4543
4545
4547
4550
4553
4554
4558
4561
4563
4568
4571
4575
4577
4580
4583
4587
4593
4597
4599
4603
4606
4608
4612
4617
4620
4623
4626
4629
4633
4635
4638
4640
4642
4644
4648
4652
4654
4656
4660
4662
4664
4667
4671
4675
4678
4681
4684
4687
4691
4694
4696
4699
4703
4707
4711
4714
4718
4722
4724
4727
4730
4734
4737
4740
4743
4746
4749
4751
4756
4759
4762
4766
4769
4772
4775
4778
4780
4783
4785
4787
4790
4794
4796
4798
4801
4804
4808
4810
4818
4821
4824
4829
4832
4836
4845
4850
4853
4855
4857
4859
4862
4864
4866
4868
4872
4874
4878
4880
4884
4889
4894
4896
4899
4902
4908
4912
4914
4917
4920
4923
4927
4930
4933
4936
4938
4940
4944
4946
4950
4953
4955
4958
4961
4963
4968
4971
4973
4976
4978
4981
4984
4987
4989
4993
4995
4997
5000
5003
5006
5008
5014
5019
5020
5023
5025
5028
5033
5035
5039
5041
5045
5049
5054
5057
5060
5065
5069
5073
5076
5079
5083
5088
5090
5092
5096
5098
5101
5105
5108
5111
5112
5115
5119
5123
5125
5128
5131
5134
5137
5140
5143
5145
5148
5151
5155
5158
5160
5164
5169
5174
5176
5179
5182
5184
5187
5192
5194
5197
5201
5204
5208
5211
5214
5217
5219
5223
5227
5230
5233
5236
5240
5244
5248
5251
5254
5259
5261
5265
5268
5271
5275
5278
5281
5284
5287
5291
5294
5296
5298
5300
5304
5307
5310
5312
5314
5319
5323
5325
5328
5330
5333
5335
5342
5349
5352
5355
5357
5359
5361
5363
5367
5369
5372
5374
5377
5381
5383
5385
5388
5390
5397
5402
5406
5410
5412
5414
5416
5419
5421
5426
5429
5432
5436
5439
5443
5452
5455
5461
5466
5471
5476
5480
5482
5486
5490
5493
5495
5499
5504
5506
5509
5512
5514
5517
5521
5525
5529
5532
5534
5536
5539
5542
5545
5550
5553
5556
5559
5562
5564
5566
5570
5572
5574
5578
5582
5585
5589
5591
5593
5596
5599
5601
5604
5607
5610
5613
5616
5619
5621
5623
5628
5633
5634
5639
5644
5648
5653
5658
5661
5666
5670
5673
5676
5682
5688
5692
5695
5699
5702
5704
5705
5710
5712
5716
5719
5722
5725
5728
5731
5733
5737
5740
5743
5746
5748
5752
5753
5756
5760
5762
5766
5768
5771
5775
5777
5779
5781
5786
5792
5798
5801
5804
5806
5809
5813
5814
5816
5819
5821
5825
5827
5831
5834
5836
5840
5842
5844
5848
5850
5853
5855
5859
5863
5868
5870
5877
5880
5885
5888
5891
5895
5898
5901
5905
5907
5912
5913
5917
5920
5923
5927
5930
5933
5941
5946
5950
5956
5960
5962
5964
5967
5969
5973
5976
5979
5981
5984
5990
5992
5995
5997
6001
6004
6009
6012
6015
6018
6021
6026
6028
6031
6034
6037
6040
6042
6045
6048
6051
6054
6057
6058
6063
6066
6069
6072
6074
6077
6081
6083
6085
6087
6091
6095
6099
6101
6106
6110
6114
6119
6122
6125
6129
6134
6139
6142
6146
6149
6151
6155
6159
6161
6164
6166
6171
6174
6176
6179
6182
6184
6186
6189
6191
6192
6195
6198
6203
6207
6210
6216
6218
6221
6223
6226
6229
6231
6234
6237
6240
6243
6247
6251
6254
6257
6260
6262
6265
6269
6271
6274
6276
6279
6282
6284
6287
6290
6292
6295
6299
6302
6305
6307
6310
6312
6316
6318
6320
6323
6327
6331
6334
6337
6340
6345
6351
6354
6358
6361
6365
6368
6370
6372
6377
6381
6386
6389
6392
6395
6398
6402
6405
6408
6411
6414
6418
6420
6423
6427
6432
6434
6437
6439
6441
6444
6448
6449
6452
6455
6457
6461
6466
6469
6474
6476
6481
6482
6484
6487
6491
6494
6496
6498
6502
6504
6507
6509
6512
6515
6520
6523
6525
6527
6529
6531
6533
6535
6538
6546
6551
6552
6556
6557
6561
6563
6567
6570
6573
6575
6579
6581
6584
6588
6590
6593
6596
6599
6603
6607
6611
6615
6618
6620
6623
6628
6630
6632
6634
6638
6642
6645
6648
6651
6656
6660
6660
6664
6666
6669
6673
6675
6679
6682
6684
6687
6691
6692
6696
6699
6701
6704
6708
6709
6713
6716
6719
6723
6725
6728
6730
6735
6738
6741
6743
6745
6750
6753
6756
6760
6762
6764
6767
6770
6771
6773
6776
6781
6784
6786
6789
6791
6796
6798
6799
6805
6809
6811
6815
6817
6822
6825
6828
6830
6833
6837
6841
6844
6847
6851
6857
6859
6863
6866
6869
6873
6875
6878
6880
6883
6886
6889
6892
6894
6896
6901
6904
6906
6910
6913
6916
6919
6922
6924
6926
6929
6932
6936
6938
6941
6944
6948
6951
6956
6959
6962
6966
6971
6975
6977
6981
6986
6989
6991
6994
6998
7002
7005
7009
7014
7018
7021
7024
7027
7030
7032
7035
7037
7043
7046
7048
7050
7053
7056
7060
7063
7065
7068
7071
7074
7078
7081
7084
7087
7089
7091
7095
7098
7101
7103
7106
7110
7113
7116
7119
7121
7123
7125
7129
7131
7134
7137
7141
7143
7145
7147
7149
7151
7157
7159
7162
7166
7169
7172
7175
7178
7181
7183
7186
7189
7193
7197
7199
7201
7203
7205
7208
7212
7214
7217
7225
7228
7231
7235
7239
7242
7245
7247
7250
7252
7255
7259
7263
7268
7270
7275
7278
7281
7285
7287
7291
7297
7299
7302
7306
7310
7312
7315
7318
7320
7323
7326
7328
7331
7335
7339
7344
7347
7352
7359
7362
7365
7371
7372
7378
7382
7386
7390
7393
7396
7399
7403
7406
7409
7415
7417
7420
7424
7427
7434
7447
7449
7456
7459
7464
7468
7468
7474
7477
7481
7485
7487
7489
7493
7496
7499
7502
7506
7508
7511
7513
7515
7518
7520
7525
7526
