pressions have to be signed some
interpretation there and they also have
to be transferred to sensory motor
interface where they can be externalized
typically in speech but does is now
known it seems comparably in sign and
may not be externalized at all its
internal thought for example I which is
the overwhelming bulk of language use
statistically speaking the so you have
some sort of generative procedure that
constructs an infinite array of
hierarchically structured expressions
and is associated with procedures to map
them onto the two interfaces well the
simplest possible operation that could
produce expressions of that kind just
takes it's going to be recursive of
course we'll take two objects already
formed and construct from them a new
object and in the simplest case it won't
it won't modify the two objects that
were put together modifying them would
require greater computational complexity
so this operation call it merge takes
two objects say x and y and what it
forms is just the set X Y ok no
modification of either x and y the just
as a matter of logic there are two
possible kinds of merge one takes x and
y which are distinct from one another
and forms the set X Y that notice of
course that if you iterate this you do
get structured expressions the
hierarchically structured expressions it
takes so it takes two objects already
constructed x and y which are distinct
from one another in forms inside XY
let's call that external merge the other
possibility is that the two objects x
and y are not distinct obviously that's
just logic and if you look at the mode
of composition it turns out that if
they're not distinct one must be
included
side the other so the second possibility
is that you merge two objects x and y
where one of them say why is inside the
other say x they're only certain y must
be of course something that had already
been constructed I call that internal
merge well so far that's just logic
internal merge if you think about its
properties yields what's called the copy
theory of movement so for example if you
which incidentally is quite well
confirmed empirically by now it that is
it's confirmed that the somatic level
it's obviously false at the sensorimotor
level so for example if you take the
hierarch expression that corresponds to
you saw what and you apply internal
merge you merge what the whole thing
what you get is what and then the set
containing what and the set constructed
from you saw or what which happens to be
the right interpretation at the semantic
level so the word what is doubly
interpreted in a way it's interpreted in
the position where you hear it as a kind
of ask opal element and it's interpreted
in its initial position where it's not
pronounced and it's there given the same
interpretation as the corresponding
sentence you saw the book or something
as the object of see with all of its
properties so the the result of internal
merge in the simplest possible form does
give the right interpretation at the
semantic interface as I say there's
plenty of evidence for that and lots of
complications and literature about it by
now but it seems to be fundamentally
right on the other hand at the
sensorimotor side it's radically wrong
so we don't say what you saw what
actually it's not totally wrong there
are cases where a residue of what's
moved does remain typically maybe only
in cases where there's some phonetic
requirement that something be there
Howard Lesnick called the keep
forgetting what you called it stranded
stranded ethics filter yeah if there's
some piece of phonetic material this has
got to be attached to something then the
smallest possible residue of what was
moved does remain and some some
languages the whole thing remains so
Chinese you just say you saw what but
interpreting it with the same Doolittle
interpretation the but anyway radically
at the finish phonetic side it's wrong
the only thing that remains is the thing
that was moved what do you see not what
did you see what well that consequence
follows from pretty Elementary
computational considerations if you were
to pronounce all of the notice this is
an artificially simple example if you
take any real example you find copies
all over the place in fact even in this
case there's internal copies but if you
were to pronounce all of those copies
that would be a tremendous computational
burden remember that the mapping to the
sensorimotor interface includes all of
phonology all of morphology and probably
even the process of linearization this
is contested but i think it's
increasingly plausible that it's true so
and that's quite a computational burden
these are the areas of language that are
highly diverse very complex affected by
historical accident so you know like the
norman conquest and so on and so forth
pitted whatever we think about it that's
a huge computational burden and language
seems to universally solve the problem
by reducing the computational burden but
of course increasing parsing difficulty
perceptual difficulty anyone who's
worked on parsing programs knows that
one of their basic problems is to try to
find work all the gaps here what at the
beginning of the sentence
you know to figure where it is I which
is a huge parsing problem so you could
say you know metaphorically that the
design of language faces too
inconsistent desiderata one to make the
use of language simple and the other to
make the design simple and universally
it chooses to make the design simple
there's quite a lot of evidence like
this that language structure is
optimized for the thought systems for
the semantic interface at the cost of
considerable complexity often at the
center of sensorimotor interface meaning
in particular for external ization and
notice that one come one aspect of
externalization is communication there's
a kind of a dogma that language is
designed for communication or evolved
fork in it for communication whatever
those phrases are supposed to mean and
maybe nothing but the dog mode to the
extent that you can make any sense of it
seems radically false it looks as if
externalization all together and
communication in particular our
ancillary processes probably both in the
design of language and probably also its
evolution issues that are well worth
pursuing there's a lot of things to say
about them but I'll put them aside just
say something just leave it at that in
any event we seem to have optimization
at the or hope for optimization
something like the SMT at the semantic
side but nothing remotely like it at the
sensorimotor side in fact the operations
that map syntactic semantic structures
on to the center remote sensorimotor
face violate every computational
principle you can think of maybe they do
the best that can be done with a hard
computational problem namely relating to
systems that have nothing to do with one
another
the sensorimotor system could have been
around for hundreds of thousands of
years before language ever emerged and
there's some but not much evidence for
Co adaptation so the whenever language
emerged you get this thought system and
somewhere down the line gets
externalized but that's a really hard
problem and it solves a lot of different
ways and as I said it's subjected to
plenty of historical accident and so on
as we know but it looks ancillary
secondary property well the notice that
internal merge is automatically
available if you take the simplest
possible computational procedure you
just get internal merge without
postulating it that's another way of
saying that is that transfer make a
simple form of transformational grammar
this is a simplified version
transformational grammar is just the
optimal system if you don't have it
you'd have to have an argument as to why
you don't there'd be an empirical burden
of proof to show that you don't have it
it's not the way things have been looked
at for the last say if you go back 50
years it was assumed that we're even
back to the late 40s when I started
working on as it was assumed that some
sort of phrase structure grammar got to
be there and then the question is do you
need other devices so my own work in the
late 40s involved discontinuous features
to try to account for long-distance
dependencies something like kind of Val
harmony carried over to the syntactic
domain by the 1950s it looked as if
transformations things which take a
piece of a structure and put it
somewhere else could be the best way of
doing that but the it was assumed at the
time by me to that phrase structure
grammar sort of given and the question
is what other devices do you need if any
well it looks as if the opposite is true
that transformations are given and you
don't need for a structure grammar by
the 1960s it had already been was
beginning to be shown that first sector
grammar is extremely complex has a lot
of stipulations and a lot of complexity
and it was shown by the 60s pretty
convincingly I think the most of this
was unnecessary actually do better
without it and by now the residues of
phrase structure grammar that remain are
very thin nothing more than merge and
some sort of algorithm may be a
universal algorithm which again has
interesting computational efficiency
properties that tells you what kind of
an object you form period doesn't even
have to be part of the grammar could be
a third factor property so phrase
structure grammar seems to have
disappeared but of at least I think so
but throughout all this period no matter
how much one tried transformation seemed
extremely resilient it seemed to be very
hard to get rid of them sensibly and I
think now we have an understanding of
why that's true transformations are just
they are free if you you have to have a
stipulation to bar them if there's some
theory of language that doesn't use
transformations it has a double
empirical burden to bear the one it has
to give empirical evidence to justify
the stipulation that you don't have
internal merge since it comes free and
secondly you need to meet the empirical
burden of showing that whatever other
devices are proposed do exist okay so as
a double empirical burden if you don't
assume transformations in the simple
form given by internal merge and
personally I don't think there's any way
to meet those two empirical burdens but
anyway that's sort of the logic of the
problem if you can't meet them it's
important for
bio linguistics well if there are two
kinds of merge and if they really are
used there's no stipulation there's no
extra complexity in the system that says
don't use one of them so the ones that
are automatically available or just used
if that's true you'd kind of expect in a
well-designed system that the two kinds
of merge have different interface
properties they work differently at the
interface well at the sensorimotor
interfaces kind of obvious internal
merge yields the ubiquitous linguistic
property of loosely called displacement
you pronounce something in one position
but you interpret it both there and
somewhere else like what did you see so
it has an obvious sensorimotor
consequence at the somatic interface it
seems there's an interesting thesis
plenty of evidence for it no not certain
I know there are people here who've
tried to deny it irrationally in my
opinion the theory won't mention any
names but the it seems to correspond
with sometimes called the duality of
semantics there's just seem to be two
different fundamentally two different
aspects to semantics one is what you get
in artificial systems invented systems
argument structure so you know agent
patient goal and so on that's one aspect
of semantics the other aspect of
semantics seems to be everything else
like information that's used for
discourse interpretation like
distinguishing new information from old
information or identifying the type of
0
1
3
6
11
14
17
19
22
25
30
33
35
37
40
46
49
53
56
58
61
64
68
71
74
77
81
84
89
91
96
98
101
103
107
109
113
115
116
121
123
127
129
130
132
133
135
138
142
145
146
149
153
155
158
161
163
167
169
171
174
177
181
183
186
189
192
195
199
201
203
205
209
212
214
218
221
226
229
231
233
235
238
240
243
245
248
251
253
258
261
264
266
268
271
273
277
279
281
283
287
290
294
296
301
305
310
313
316
321
323
326
329
332
334
336
339
342
345
348
350
354
356
358
363
366
369
372
374
377
381
384
386
388
392
395
396
399
402
405
407
412
416
419
421
423
426
430
433
436
438
440
446
449
452
455
457
460
464
466
469
471
473
477
479
484
486
488
491
493
496
498
501
504
508
510
514
517
521
524
525
526
528
530
532
536
540
545
547
549
552
554
557
560
569
572
573
576
578
585
587
589
590
593
597
598
600
604
606
609
612
614
617
619
621
625
630
631
634
636
641
643
644
647
652
655
656
659
662
666
669
671
674
676
679
683
685
686
690
692
697
700
702
704
707
711
712
715
716
719
722
725
727
730
731
734
737
740
743
746
749
753
756
759
762
764
769
771
775
778
780
784
786
788
790
794
796
799
802
805
807
811
813
815
818
820
823
828
830
832
834
838
840
845
847
850
853
856
858
863
865
867
869
873
876
880
884
887
888
891
894
896
